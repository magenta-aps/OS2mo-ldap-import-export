# SPDX-FileCopyrightText: 2019-2020 Magenta ApS
# SPDX-License-Identifier: MPL-2.0
"""Dataloaders to bulk requests."""
import asyncio
from contextlib import suppress
from datetime import datetime
from datetime import timezone
from enum import Enum
from enum import auto
from functools import partialmethod
from functools import wraps
from typing import Any
from typing import Literal
from typing import Protocol
from typing import TypeVar
from typing import cast
from uuid import UUID

import structlog
from fastapi.encoders import jsonable_encoder
from ldap3 import BASE
from ldap3.core.exceptions import LDAPInvalidValueError
from ldap3.protocol import oid
from ldap3.utils.dn import safe_dn
from ldap3.utils.dn import to_dn
from more_itertools import bucket
from more_itertools import one
from more_itertools import only
from more_itertools import partition
from ramodels.mo import MOBase
from ramodels.mo._shared import validate_cpr
from ramodels.mo.details.address import Address
from ramodels.mo.details.engagement import Engagement
from ramodels.mo.details.it_system import ITUser
from ramodels.mo.employee import Employee
from ramodels.mo.organisation_unit import OrganisationUnit

from .autogenerated_graphql_client import GraphQLClient
from .autogenerated_graphql_client.base_model import UNSET
from .autogenerated_graphql_client.input_types import AddressTerminateInput
from .autogenerated_graphql_client.input_types import ClassCreateInput
from .autogenerated_graphql_client.input_types import EmployeeFilter
from .autogenerated_graphql_client.input_types import EngagementFilter
from .autogenerated_graphql_client.input_types import EngagementTerminateInput
from .autogenerated_graphql_client.input_types import ITUserTerminateInput
from .autogenerated_graphql_client.input_types import RAOpenValidityInput
from .config import Settings
from .environments import filter_remove_curly_brackets
from .exceptions import AttributeNotFound
from .exceptions import DNNotFound
from .exceptions import InvalidChangeDict
from .exceptions import MultipleObjectsReturnedException
from .exceptions import NoObjectsReturnedException
from .exceptions import ReadOnlyException
from .exceptions import UUIDNotFoundException
from .ldap import get_attribute_types
from .ldap import get_ldap_attributes
from .ldap import get_ldap_object
from .ldap import get_ldap_schema
from .ldap import get_ldap_superiors
from .ldap import is_uuid
from .ldap import ldap_add
from .ldap import ldap_compare
from .ldap import ldap_delete
from .ldap import ldap_modify
from .ldap import ldap_modify_dn
from .ldap import make_ldap_object
from .ldap import object_search
from .ldap import paged_search
from .ldap import single_object_search
from .ldap_classes import LdapObject
from .types import DN
from .types import CPRNumber
from .types import OrgUnitUUID
from .usernames import UserNameGenerator
from .utils import combine_dn_strings
from .utils import extract_cn_from_dn
from .utils import extract_ou_from_dn
from .utils import is_exception
from .utils import remove_cn_from_dn

logger = structlog.stdlib.get_logger()


class Verb(Enum):
    CREATE = auto()
    EDIT = auto()
    TERMINATE = auto()


class Validity(Protocol):
    @property
    def from_(self) -> datetime | None:  # pragma: no cover
        ...

    @property
    def to(self) -> datetime | None:  # pragma: no cover
        ...


class ValidityModel(Protocol):
    @property
    def validity(self) -> Validity:  # pragma: no cover
        ...


T = TypeVar("T", bound=ValidityModel)


def extract_current_or_latest_validity(validities: list[T]) -> T | None:
    """
    Check each validity in a list of validities and return the one which is either
    valid today, or has the latest end-date
    """
    if len(validities) < 2:
        return only(validities)

    def is_current(val: T) -> bool:
        # Cannot use datetime.utcnow as it is not timezone aware
        now_utc = datetime.now(timezone.utc)

        match (val.validity.from_, val.validity.to):
            case (None, None):
                return True
            case (start, None):
                assert start is not None
                return start < now_utc
            case (None, end):
                assert end is not None
                return now_utc < end
            case (start, end):
                assert start is not None
                assert end is not None
                return start < now_utc and now_utc < end
            case _:  # pragma: no cover
                assert False

    # If any of the validities is valid today, return it
    current_validity = only(filter(is_current, validities))
    if current_validity:
        return current_validity
    # Otherwise return the latest
    # TODO: Does this actually make sense? - Should we not return the one which is the
    #       closest to now, rather than the one that is the furthest into the future?
    # Cannot use datetime.max directly as it is not timezone aware
    datetime_max_utc = datetime.max.replace(tzinfo=timezone.utc)
    latest_validity = max(
        validities, key=lambda val: val.validity.to or datetime_max_utc
    )
    return latest_validity


class DataLoader:
    def __init__(self, context):
        self.context = context
        self.user_context = context["user_context"]
        self.ldap_connection = self.user_context["ldap_connection"]
        self.attribute_types = get_attribute_types(self.ldap_connection)
        self.single_value = {k: v.single_value for k, v in self.attribute_types.items()}
        self._mo_to_ldap_attributes = []
        self.create_mo_class_lock = asyncio.Lock()

        # Relate graphQL object types (left) to AMQP routing key object types (right)
        self.object_type_dict = {
            "employees": "person",
            "org_units": "org_unit",
            "addresses": "address",
            "itusers": "ituser",
            "engagements": "engagement",
        }

        self.object_type_dict_inv = {
            str(v): k for k, v in self.object_type_dict.items()
        }

        self.supported_object_types = list(self.object_type_dict_inv.keys())

    @property
    def graphql_client(self) -> GraphQLClient:
        return cast(GraphQLClient, self.context["graphql_client"])

    @property
    def sync_tool(self):
        from .import_export import SyncTool

        return cast(SyncTool, self.user_context["sync_tool"])

    @property
    def mo_to_ldap_attributes(self):
        """
        Populates self._mo_to_ldap_attributes and returns it.

        self._mo_to_ldap_attributes is a list of all LDAP attribute names which
        are synchronized to LDAP

        Notes
        -------
        This is not done in __init__() because the converter is not initialized yet,
        when we initialize the dataloader.
        """
        if not self._mo_to_ldap_attributes:
            converter = self.user_context["converter"]
            for json_dict in converter.mapping["mo_to_ldap"].values():
                self._mo_to_ldap_attributes.extend(list(json_dict.keys()))
        return self._mo_to_ldap_attributes

    def shared_attribute(self, attribute: str):
        """
        Determine if an attribute is shared between multiple LDAP objects.

        Parameters
        ------------
        attribute : str
            LDAP attribute name

        Returns
        ----------
        return_value : bool
            True if the attribute is shared between different LDAP objects, False if it
            is not.

        Examples
        -----------
        >>> self.shared_attribute("cpr_no")
        >>> True

        The "cpr_no" attribute is generally shared between different LDAP objects.
        Therefore the return value is "True"

        >>> self.shared_attribute("mobile_phone_no")
        >>> False

        An attribute which contains a phone number is generally only used by a single
        LDAP object. Therefore the return value is "False"

        Notes
        -------
        The return value in above examples depends on the json dictionary.
        """
        occurences = self.mo_to_ldap_attributes.count(attribute)
        if occurences == 1:
            return False
        elif occurences > 1:
            return True
        else:
            raise AttributeNotFound(
                f"'{attribute}' not found in 'mo_to_ldap' attributes"
            )

    async def load_ldap_object(
        self,
        dn: DN,
        attributes: list | None,
        nest: bool = True,
    ) -> LdapObject:  # pragma: no cover
        # TODO: Actually eliminate this function by calling get_ldap_object directly.
        #       Be warned though, doing so breaks ~25 tests because of bad mocking.
        ldap_connection = self.context["user_context"]["ldap_connection"]
        return await get_ldap_object(dn, ldap_connection, nest, attributes)

    async def load_ldap_cpr_object(
        self,
        cpr_no: CPRNumber,
        json_key: str,
        additional_attributes: list[str] = [],
    ) -> list[LdapObject]:
        """
        Loads an ldap object which can be found using a cpr number lookup

        Accepted json_keys are:
            - 'Employee'
            - a MO address type name
        """
        try:
            validate_cpr(cpr_no)
        except (ValueError, TypeError):
            raise NoObjectsReturnedException(f"cpr_no '{cpr_no}' is invalid")

        cpr_field = self.user_context["cpr_field"]
        if not cpr_field:
            raise NoObjectsReturnedException("cpr_field is not configured")

        settings = self.user_context["settings"]

        search_base = settings.ldap_search_base
        ous_to_search_in = settings.ldap_ous_to_search_in
        search_bases = [
            combine_dn_strings([ou, search_base]) for ou in ous_to_search_in
        ]
        converter = self.user_context["converter"]

        object_class = converter.find_ldap_object_class(json_key)
        attributes = converter.get_ldap_attributes(json_key) + additional_attributes

        object_class_filter = f"objectclass={object_class}"
        cpr_filter = f"{cpr_field}={cpr_no}"

        searchParameters = {
            "search_base": search_bases,
            "search_filter": f"(&({object_class_filter})({cpr_filter}))",
            "attributes": list(set(attributes)),
        }
        ldap_connection = self.context["user_context"]["ldap_connection"]
        search_results = await object_search(searchParameters, ldap_connection)
        # TODO: Asyncio gather this
        ldap_objects: list[LdapObject] = [
            await make_ldap_object(search_result, self.context)
            for search_result in search_results
        ]
        dns = [obj.dn for obj in ldap_objects]
        logger.info("Found LDAP(s) object", dns=dns)
        return ldap_objects

    def ou_in_ous_to_write_to(self, dn: str) -> bool:
        """
        Determine if an OU is among those to which we are allowed to write.
        """
        settings = self.user_context["settings"]

        if "" in settings.ldap_ous_to_write_to:
            # Empty string means that it is allowed to write to all OUs
            return True

        ou = extract_ou_from_dn(dn)
        ous_to_write_to = [safe_dn(ou) for ou in settings.ldap_ous_to_write_to]
        for ou_to_write_to in ous_to_write_to:
            if ou.endswith(ou_to_write_to):
                # If an OU ends with one of the OUs-to-write-to, it's OK.
                # For example, if we are only allowed to write to "OU=foo",
                # Then we are also allowed to write to "OU=bar,OU=foo", which is a
                # sub-OU inside "OU=foo"
                return True

        logger.info("OU not in OUs to write", ou=ou, ous_to_write_to=ous_to_write_to)
        return False

    async def modify_ldap(
        self,
        operation: Literal[
            "MODIFY_ADD", "MODIFY_DELETE", "MODIFY_REPLACE", "MODIFY_INCREMENT"
        ],
        dn: str,
        attribute: str,
        value: list[str] | str,
    ) -> dict | None:
        """
        Modifies LDAP
        """
        # TODO: Remove this when ldap3s read-only flag works
        settings = self.user_context["settings"]
        if settings.ldap_read_only:
            logger.info(
                "LDAP connection is read-only",
                operation="modify_ldap",
                dn=dn,
                attribute=attribute,
            )
            raise ReadOnlyException("LDAP connection is read-only")

        # Checks
        if not self.ou_in_ous_to_write_to(dn):
            return None

        if isinstance(value, list):
            value = only(
                value,
                default="",
                too_long=InvalidChangeDict(
                    "Exactly one value can be changed at a time"
                ),
            )

        # Compare to LDAP
        value_exists = await ldap_compare(self.ldap_connection, dn, attribute, value)

        # If the value is already as expected, and we are not deleting, we are done
        if value_exists and "DELETE" not in operation:
            logger.info(
                "Attribute value already exists",
                attribute=attribute,
                value_to_modify=value,
            )
            return None

        # Modify LDAP
        changes = {attribute: [(operation, value)]}
        logger.info("Uploading the changes", changes=changes, dn=dn)
        _, result = await ldap_modify(self.ldap_connection, dn, changes)
        logger.info("LDAP Result", result=result, dn=dn)
        return result

    add_ldap = partialmethod(modify_ldap, "MODIFY_ADD")
    delete_ldap = partialmethod(modify_ldap, "MODIFY_DELETE")
    replace_ldap = partialmethod(modify_ldap, "MODIFY_REPLACE")

    async def load_ldap_OUs(self, search_base: str | None = None) -> dict:
        """
        Returns a dictionary where the keys are OU strings and the items are dicts
        which contain information about the OU
        """
        searchParameters: dict = {
            "search_filter": "(objectclass=OrganizationalUnit)",
            "attributes": [],
        }

        responses = await paged_search(
            self.context,
            searchParameters,
            search_base=search_base,
            mute=True,
        )

        dns = [r["dn"] for r in responses]
        output = {}

        for dn in dns:
            searchParameters = {
                "search_filter": "(objectclass=user)",
                "attributes": [],
                "size_limit": 1,
            }

            responses = await paged_search(
                self.context,
                searchParameters,
                search_base=dn,
                mute=True,
            )
            ou = extract_ou_from_dn(dn)
            if len(responses) == 0:
                output[ou] = {"empty": True}
            else:
                output[ou] = {"empty": False}
            output[ou]["dn"] = dn

        return output

    async def add_ldap_object(self, dn: str, attributes: dict[str, Any] | None = None):
        """
        Adds a new object to LDAP

        Parameters
        ---------------
        attributes : dict
            dictionary with attributes to populate in LDAP, when creating the user.
            See https://ldap3.readthedocs.io/en/latest/add.html for more information

        """
        settings: Settings = self.user_context["settings"]
        # TODO: Remove this when ldap3s read-only flag works
        if settings.ldap_read_only:
            logger.info(
                "LDAP connection is read-only",
                operation="add_ldap_object",
                dn=dn,
                attributes=attributes,
            )
            raise ReadOnlyException("LDAP connection is read-only")

        if not settings.add_objects_to_ldap:
            logger.info(
                "Adding LDAP objects is disabled",
                operation="add_ldap_object",
                dn=dn,
                attributes=attributes,
            )
            raise ReadOnlyException("Adding LDAP objects is disabled")

        if not self.ou_in_ous_to_write_to(dn):
            logger.info(
                "Not allowed to write to the specified OU",
                operation="add_ldap_object",
                dn=dn,
                attributes=attributes,
            )
            raise ReadOnlyException("Not allowed to write to the specified OU")

        logger.info("Adding user to LDAP", dn=dn, attributes=attributes)
        _, result = await ldap_add(
            self.ldap_connection,
            dn,
            self.user_context["converter"].find_ldap_object_class("Employee"),
            attributes=attributes,
        )
        logger.info("LDAP Result", result=result, dn=dn)

    @staticmethod
    def decompose_ou_string(ou: str) -> list[str]:
        """
        Decomposes an OU string and returns a list of OUs where the first one is the
        given OU string, and the last one if the highest parent OU

        Example
        -----------
        >>> ou = 'OU=foo,OU=bar'
        >>> decompose_ou_string(ou)
        >>> ['OU=foo,OU=bar', 'OU=bar']
        """

        ou_parts = to_dn(ou)
        output = []
        for i in range(len(ou_parts)):
            output.append(combine_dn_strings(ou_parts[i:]))

        return output

    async def create_ou(self, ou: str) -> None:
        """
        Creates an OU. If the parent OU does not exist, creates that one first
        """
        settings = self.user_context["settings"]

        # TODO: Remove this when ldap3s read-only flag works
        if settings.ldap_read_only:
            logger.info("LDAP connection is read-only", operation="create_ou", ou=ou)
            raise ReadOnlyException("LDAP connection is read-only")

        if not settings.add_objects_to_ldap:
            logger.info("Adding LDAP objects is disabled", operation="create_ou", ou=ou)
            raise ReadOnlyException("Adding LDAP objects is disabled")

        if not self.ou_in_ous_to_write_to(ou):
            return

        # TODO: Search for specific OUs as needed instead of reading all of LDAP?
        ou_dict = await self.load_ldap_OUs()

        # Create OUs top-down (unless they already exist)
        for ou_to_create in self.decompose_ou_string(ou)[::-1]:
            if ou_to_create not in ou_dict:
                logger.info("Creating OU", ou_to_create=ou_to_create)
                dn = combine_dn_strings([ou_to_create, settings.ldap_search_base])
                _, result = await ldap_add(
                    self.ldap_connection, dn, "OrganizationalUnit"
                )
                logger.info("LDAP Result", result=result, dn=dn)

    async def delete_ou(self, ou: str) -> None:
        """
        Deletes an OU. If the parent OU is empty after deleting, also deletes that one

        Notes
        --------
        Only deletes OUs which are empty
        """
        settings = self.user_context["settings"]
        # TODO: Remove this when ldap3s read-only flag works
        if settings.ldap_read_only:
            logger.info("LDAP connection is read-only", operation="delete_ou", ou=ou)
            raise ReadOnlyException("LDAP connection is read-only")

        if not self.ou_in_ous_to_write_to(ou):
            return

        for ou_to_delete in self.decompose_ou_string(ou):
            # TODO: Search for specific OUs as needed instead of reading all of LDAP?
            ou_dict = await self.load_ldap_OUs()
            if (
                ou_dict.get(ou_to_delete, {}).get("empty", False)
                and ou_to_delete != settings.ldap_ou_for_new_users
            ):
                logger.info("Deleting OU", ou_to_delete=ou_to_delete)
                dn = combine_dn_strings([ou_to_delete, settings.ldap_search_base])
                _, result = await ldap_delete(self.ldap_connection, dn)
                logger.info("LDAP Result", result=result, dn=dn)

    async def move_ldap_object(self, old_dn: str, new_dn: str) -> bool:
        """
        Moves an LDAP object from one DN to another. Returns True if the move was
        successful.
        """
        settings = self.user_context["settings"]

        # TODO: Remove this when ldap3s read-only flag works
        if settings.ldap_read_only:
            logger.info(
                "LDAP connection is read-only",
                operation="move_ldap_object",
                old_dn=old_dn,
                new_dn=new_dn,
            )
            raise ReadOnlyException("LDAP connection is read-only")

        if not settings.add_objects_to_ldap:
            logger.info(
                "Adding LDAP objects is disabled",
                operation="move_ldap_object",
                old_dn=old_dn,
                new_dn=new_dn,
            )
            raise ReadOnlyException("Adding LDAP objects is disabled")

        if not self.ou_in_ous_to_write_to(new_dn):
            return False

        logger.info("Moving entry", old_dn=old_dn, new_dn=new_dn)

        _, result = await ldap_modify_dn(
            self.ldap_connection,
            old_dn,
            extract_cn_from_dn(new_dn),
            new_superior=remove_cn_from_dn(new_dn),
        )
        logger.info("LDAP Result", result=result, new_dn=new_dn, old_dn=old_dn)
        return True if result["description"] == "success" else False

    async def modify_ldap_object(
        self,
        object_to_modify: LdapObject,
        json_key: str,
        overwrite: bool = False,
        delete: bool = False,
    ) -> list[dict]:
        """
        Parameters
        -------------
        object_to_modify : LDAPObject
            object to upload to LDAP
        json_key : str
            json key to upload. e.g. 'Employee' or 'Engagement' or another key present
            in the json dictionary.
        overwrite: bool
            Set to True to overwrite contents in LDAP
        delete: bool
            Set to True to delete contents in LDAP, instead of creating/modifying them
        """
        converter = self.user_context["converter"]
        if not converter._export_to_ldap_(json_key):
            logger.info("_export_to_ldap_ == False.", json_key=json_key)
            return []
        success = 0
        failed = 0

        parameters_to_modify = list(object_to_modify.dict().keys())

        logger.info("Uploading object", object_to_modify=object_to_modify)
        parameters_to_modify = [p for p in parameters_to_modify if p != "dn"]
        dn = object_to_modify.dn
        results = []

        if delete:
            # Only delete parameters which are not shared between different objects.
            # For example: 'org-unit name' should not be deleted if both
            # engagements and org unit addresses use it;
            #
            # If we would delete 'org-unit name' as a part of an org-unit address delete
            # operation, We would suddenly not be able to import engagements any more.

            # TODO: This means that any attribute referenced by multiple templates will
            #       literally never be cleared. This seems potentially highly problematic?
            #       We should probably consider if this is the optimal design.
            #
            #       I have discussed this with Casper, and the solution seems to be that
            #       we have to synchronize the entire state at once, otherwise we cannot
            #       possibly ensure deletions work as expected. This is also what the
            #       Omada integration does.
            parameters_to_modify = [
                p for p in parameters_to_modify if not self.shared_attribute(p)
            ]

        for parameter_to_modify in parameters_to_modify:
            value = getattr(object_to_modify, parameter_to_modify)
            value_to_modify: list[str] = [] if value is None else [value]

            operation = None
            if delete:
                operation = self.delete_ldap
            elif self.single_value[parameter_to_modify] or overwrite:
                operation = self.replace_ldap
            else:
                operation = self.add_ldap

            try:
                response = await operation(dn, parameter_to_modify, value_to_modify)
            except LDAPInvalidValueError:
                logger.warning("LDAPInvalidValueError exception", exc_info=True)
                failed += 1
                continue

            if response and response["description"] == "success":
                success += 1
            elif response:
                failed += 1

            if response:
                results.append(response)

        logger.info(
            "Succeeded/failed MODIFY_* operations",
            success=success,
            failed=failed,
        )

        return results

    def make_overview_entry(self, attributes, superiors, example_value_dict=None):
        attribute_dict = {}
        for attribute in attributes:
            # skip unmapped types
            if attribute not in self.attribute_types:
                continue
            syntax = self.attribute_types[attribute].syntax

            # decoded syntax tuple structure: (oid, kind, name, docs)
            syntax_decoded = oid.decode_syntax(syntax)
            details_dict = {
                "single_value": self.attribute_types[attribute].single_value,
                "syntax": syntax,
            }
            if syntax_decoded:
                details_dict["field_type"] = syntax_decoded[2]

            if example_value_dict:
                if attribute in example_value_dict:
                    details_dict["example_value"] = example_value_dict[attribute]

            attribute_dict[attribute] = details_dict

        return {
            "superiors": superiors,
            "attributes": attribute_dict,
        }

    def load_ldap_overview(self):
        schema = get_ldap_schema(self.ldap_connection)

        all_object_classes = sorted(list(schema.object_classes.keys()))

        output = {}
        for ldap_class in all_object_classes:
            all_attributes = get_ldap_attributes(self.ldap_connection, ldap_class)
            superiors = get_ldap_superiors(self.ldap_connection, ldap_class)
            output[ldap_class] = self.make_overview_entry(all_attributes, superiors)

        return output

    async def find_mo_employee_uuid_via_cpr_number(self, dn: str) -> set[UUID]:
        cpr_field = self.user_context["cpr_field"]
        if cpr_field is None:
            return set()

        ldap_object = await self.load_ldap_object(dn, [cpr_field])
        # Try to get the cpr number from LDAP and use that.
        try:
            raw_cpr_no = getattr(ldap_object, cpr_field)
            # NOTE: Not sure if this only necessary for the mocked server or not
            if isinstance(raw_cpr_no, list):
                raw_cpr_no = one(raw_cpr_no)
            cpr_no = validate_cpr(str(raw_cpr_no))
            assert cpr_no is not None
            cpr_number = CPRNumber(cpr_no)
        except ValueError:
            return set()

        result = await self.graphql_client.read_employee_uuid_by_cpr_number(cpr_number)
        return {employee.uuid for employee in result.objects}

    async def find_mo_employee_uuid_via_ituser(self, dn: str) -> set[UUID]:
        unique_uuid = await self.get_ldap_unique_ldap_uuid(dn)
        result = await self.graphql_client.read_employee_uuid_by_ituser_user_key(
            str(unique_uuid)
        )
        return {
            ituser.current.employee_uuid
            for ituser in result.objects
            if ituser.current is not None and ituser.current.employee_uuid is not None
        }

    async def find_mo_employee_uuid(self, dn: str) -> UUID | None:
        cpr_results = await self.find_mo_employee_uuid_via_cpr_number(dn)
        if len(cpr_results) == 1:
            uuid = one(cpr_results)
            logger.info("Found employee via CPR matching", dn=dn, uuid=uuid)
            return uuid

        ituser_results = await self.find_mo_employee_uuid_via_ituser(dn)
        if len(ituser_results) == 1:
            uuid = one(ituser_results)
            logger.info("Found employee via ITUser matching", dn=dn, uuid=uuid)
            return uuid

        # TODO: Return an ExceptionGroup with both
        # NOTE: This may break a lot of things, because we explicitly match against MultipleObjectsReturnedException
        if len(cpr_results) > 1:
            raise MultipleObjectsReturnedException(f"Multiple CPR matches for dn={dn}")

        if len(ituser_results) > 1:
            raise MultipleObjectsReturnedException(
                f"Multiple ITUser matches for dn={dn}"
            )

        logger.info("No matching employee", dn=dn)
        return None

    async def find_mo_engagement_uuid(self, dn: DN) -> None | UUID:
        # Get Unique LDAP UUID from DN, then get engagement by looking for IT user with that
        # Unique LDAP UUID in MO.

        settings = self.user_context["settings"]
        ldap_object = await self.load_ldap_object(dn, [settings.ldap_unique_id_field])
        raw_unique_uuid = getattr(ldap_object, settings.ldap_unique_id_field)
        # NOTE: Not sure if this only necessary for the mocked server or not
        if isinstance(raw_unique_uuid, list):
            raw_unique_uuid = one(raw_unique_uuid)
        unique_uuid = filter_remove_curly_brackets(raw_unique_uuid)

        itsystem_uuid = await self.get_ldap_it_system_uuid()
        if itsystem_uuid is None:
            logger.info(
                "Could not find engagement UUID for DN",
                dn=dn,
                unique_ldap_uuid=unique_uuid,
                itsystem_uuid=itsystem_uuid,
            )
            return None

        result = await self.graphql_client.read_engagement_uuid_by_ituser_user_key(
            unique_uuid, UUID(itsystem_uuid)
        )
        engagement_uuids = {
            ituser.current.engagement_uuid
            for ituser in result.objects
            if ituser.current is not None
        }
        engagement_uuid = only(engagement_uuids)
        if engagement_uuid is None:
            logger.info(
                "Could not find engagement UUID for DN",
                dn=dn,
                unique_ldap_uuid=unique_uuid,
                itsystem_uuid=itsystem_uuid,
                engagement_uuid=engagement_uuid,
            )
            return None

        return engagement_uuid

    async def get_ldap_it_system_uuid(self) -> str | None:
        """
        Return the IT system uuid belonging to the LDAP-it-system
        Return None if the LDAP-it-system is not found.
        """
        user_key = self.user_context["ldap_it_system_user_key"]
        if user_key is None:
            return None

        try:
            from .converters import get_it_system_uuid

            return await get_it_system_uuid(self.graphql_client, user_key)
        except UUIDNotFoundException:
            logger.info(
                "UUID Not found",
                suggestion=f"Does the '{user_key}' it-system exist?",
            )
            return None

    async def get_ldap_dn(self, unique_ldap_uuid: UUID) -> DN:
        """
        Given an unique_ldap_uuid, find the DistinguishedName
        """
        logger.info("Looking for LDAP object", unique_ldap_uuid=unique_ldap_uuid)
        settings = self.user_context["settings"]
        searchParameters = {
            "search_base": settings.ldap_search_base,
            "search_filter": f"(&(objectclass=*)({settings.ldap_unique_id_field}={unique_ldap_uuid}))",
            "attributes": [],
        }

        # Special-case for AD
        if settings.ldap_unique_id_field == "objectGUID":
            searchParameters = {
                "search_base": f"<GUID={unique_ldap_uuid}>",
                "search_filter": "(objectclass=*)",
                "attributes": [],
                "search_scope": BASE,
            }

        ldap_connection = self.context["user_context"]["ldap_connection"]
        search_result = await single_object_search(searchParameters, ldap_connection)
        dn: str = search_result["dn"]
        return dn

    async def get_ldap_unique_ldap_uuid(self, dn: str) -> UUID:
        """
        Given a DN, find the unique_ldap_uuid
        """
        settings = self.user_context["settings"]
        logger.info("Looking for LDAP object", dn=dn)
        ldap_object = await self.load_ldap_object(dn, [settings.ldap_unique_id_field])
        uuid = getattr(ldap_object, settings.ldap_unique_id_field)
        if not uuid:
            # Some computer-account objects has no samaccountname
            raise NoObjectsReturnedException(
                f"Object has no {settings.ldap_unique_id_field}"
            )
        return UUID(uuid)

    def extract_unique_ldap_uuids(self, it_users: list[ITUser]) -> set[UUID]:
        """
        Extracts unique ldap uuids from a list of it-users
        """
        it_user_keys = {ituser.user_key for ituser in it_users}
        not_uuids, uuids = partition(is_uuid, it_user_keys)
        for user_key in not_uuids:
            # TODO: Make this an exception instead of just ignoring bad values
            logger.warning(
                "IT-user is not a UUID",
                user_key=user_key,
            )
        # TODO: Check for duplicates?
        return set(map(UUID, uuids))

    async def convert_ldap_uuids_to_dns(self, ldap_uuids: set[UUID]) -> set[DN]:
        # TODO: DataLoader / bulk here instead of this
        results = await asyncio.gather(
            *[self.get_ldap_dn(uuid) for uuid in ldap_uuids], return_exceptions=True
        )
        exceptions = cast(list[Exception], list(filter(is_exception, results)))
        if exceptions:
            raise ExceptionGroup("Exceptions during UUID2DN translation", exceptions)
        return cast(set[DN], set(results))

    async def find_mo_employee_dn_by_itsystem(self, uuid: UUID) -> set[DN]:
        """Tries to find the LDAP DNs belonging to a MO employee via ITUsers.

        Args:
            uuid: UUID of the employee to try to find DNs for.

        Returns:
            A potentially empty set of DNs.
        """
        # TODO: How do we know if the ITUser is up-to-date with the newest DNs in AD?

        # The ITSystem only exists if configured to do so
        raw_it_system_uuid = await self.get_ldap_it_system_uuid()
        # If it does not exist, we cannot fetch users for it
        if raw_it_system_uuid is None:
            return set()

        it_system_uuid = UUID(raw_it_system_uuid)
        it_users = await self.load_mo_employee_it_users(uuid, it_system_uuid)
        ldap_uuids = self.extract_unique_ldap_uuids(it_users)
        dns = await self.convert_ldap_uuids_to_dns(ldap_uuids)
        # No DNs, no problem
        if not dns:
            return set()

        # If we have one or more ITUsers (with valid dns), return those
        logger.info(
            "Found DN(s) using ITUser lookup",
            dns=dns,
            employee_uuid=uuid,
        )
        return dns

    async def find_mo_employee_dn_by_cpr_number(self, uuid: UUID) -> set[DN]:
        """Tries to find the LDAP DNs belonging to a MO employee via CPR numbers.

        Args:
            uuid: UUID of the employee to try to find DNs for.

        Returns:
            A potentially empty set of DNs.
        """
        # If the employee has a cpr-no, try using that to find matchind DNs
        employee = await self.load_mo_employee(uuid)
        if employee is None:
            raise NoObjectsReturnedException(f"Unable to lookup employee: {uuid}")
        cpr_no = CPRNumber(employee.cpr_no) if employee.cpr_no else None
        # No CPR, no problem
        if not cpr_no:
            return set()

        logger.info(
            "Attempting CPR number lookup",
            employee_uuid=uuid,
        )
        dns = set()
        with suppress(NoObjectsReturnedException):
            dns = {
                obj.dn for obj in await self.load_ldap_cpr_object(cpr_no, "Employee")
            }
        if not dns:
            return set()
        logger.info(
            "Found DN(s) using CPR number lookup",
            dns=dns,
            employee_uuid=uuid,
        )
        return dns

    async def find_mo_employee_dn(self, uuid: UUID) -> set[DN]:
        """Tries to find the LDAP DNs belonging to a MO employee.

        Args:
            uuid: UUID of the employee to try to find DNs for.

        Returns:
            A potentially empty set of DNs.
        """
        # TODO: This should probably return a list of EntityUUIDs rather than DNs
        #       However this should probably be a change away from DNs in general
        logger.info(
            "Attempting to find DNs",
            employee_uuid=uuid,
        )
        # TODO: We should be able to trust just the ITUsers, however we do not.
        #       Maybe once the code becomes easier to reason about, we can get to that.
        #       But for now, we fetch all accounts, and use the discriminator.
        #
        # TODO: We may want to expand this in the future to also check for half-created
        #       objects, to support scenarios where the application may crash after
        #       creating an LDAP account, but before making a MO ITUser.
        ituser_dns, cpr_number_dns = await asyncio.gather(
            self.find_mo_employee_dn_by_itsystem(uuid),
            self.find_mo_employee_dn_by_cpr_number(uuid),
        )
        dns = ituser_dns | cpr_number_dns
        if dns:
            return dns
        logger.warning(
            "Unable to find DNs for MO employee",
            employee_uuid=uuid,
        )
        return set()

    async def make_mo_employee_dn(self, uuid: UUID) -> DN:
        employee = await self.load_mo_employee(uuid)
        if employee is None:
            raise NoObjectsReturnedException(f"Unable to lookup employee: {uuid}")
        cpr_no = CPRNumber(employee.cpr_no) if employee.cpr_no else None

        # Check if we even dare create a DN
        raw_it_system_uuid = await self.get_ldap_it_system_uuid()
        if raw_it_system_uuid is None and cpr_no is None:
            logger.warning(
                "Could not or generate a DN for employee (cannot correlate)",
                employee_uuid=uuid,
            )
            raise DNNotFound("Unable to generate DN, no correlation key available")

        # If we did not find a DN neither via ITUser nor via CPR-number, then we want
        # to create one, by generating a DN, importing the user and potentially creating
        # a binding between the two.
        username_generator: UserNameGenerator = self.user_context["username_generator"]

        logger.info("Generating DN for user", employee_uuid=uuid)
        # NOTE: This not only generates the DN as the name suggests,
        #       rather it also *creates it in LDAP*, be warned!
        #
        #       Additionally it turns out that it does not only create the DN in LDAP
        #       rather it uploads the entire employee object to LDAP.
        #
        # TODO: Does this upload actively require a cpr_no on the employee?
        #       If we do not have the CPR number nor the ITSystem, we would be leaking
        #       the DN we generate, so maybe we should guard for this, the old code seemed
        #       to do so, maybe we should simply not upload anything in that case.
        dn = await username_generator.generate_dn(employee)

        # If the LDAP ITSystem exists, we want to create a binding to our newly
        # generated (and created) DN, such that it can be correlated in the future.
        #
        # NOTE: This may not be executed if the program crashes after the above line,
        #       thus the current code is not robust and may fail at any time.
        #       The appropriate solution here is to ensure that generate_dn atomically
        #       creates a link between the MO entity and the newly created LDAP entity,
        #       such as by adding the MO UUID to the newly created LDAP entity.
        if raw_it_system_uuid is not None:
            logger.info(
                "No ITUser found, creating one to correlate with DN",
                employee_uuid=uuid,
                dn=dn,
            )
            # Get its unique ldap uuid
            # TODO: Get rid of this code and operate on EntityUUIDs thoughout
            unique_uuid = await self.get_ldap_unique_ldap_uuid(dn)
            logger.info(
                "LDAP UUID found for DN",
                employee_uuid=uuid,
                dn=dn,
                ldap_uuid=unique_uuid,
            )
            # Make a new it-user
            it_user = ITUser.from_simplified_fields(
                str(unique_uuid),
                UUID(raw_it_system_uuid),
                datetime.today().strftime("%Y-%m-%d"),
                person_uuid=uuid,
            )
            await self.create_ituser(it_user)

        # TODO: What is this purpose of this import, if we just created the DN,
        #       the data should already be up-to-date, no?
        #       It seems weird to synchronize back and forth immediately, but maybe it
        #       is just because the create by generate_dn does not in fact create it
        #       correctly?
        # TODO: Publish this message on the LDAP AMQP exchange
        await self.sync_tool.import_single_user(dn, manual_import=True)
        await self.graphql_client.employee_refresh(
            self.sync_tool.amqpsystem.exchange_name, [employee.uuid]
        )
        return dn

    async def load_mo_employee(
        self, uuid: UUID, current_objects_only=True
    ) -> Employee | None:
        start = end = UNSET if current_objects_only else None
        results = await self.graphql_client.read_employees([uuid], start, end)
        result = only(results.objects)
        if result is None:
            return None
        result_entry = extract_current_or_latest_validity(result.validities)
        if result_entry is None:
            return None
        entry = jsonable_encoder(result_entry)
        entry.pop("validity")
        return Employee(**entry)

    async def load_mo_class_uuid(self, user_key: str) -> UUID | None:
        """Find the UUID of a class by user-key.

        Args:
            user_key: The user-key to lookup.

        Raises:
            MultipleObjectsReturnedException:
                If multiple classes share the same user-key.

        Returns:
            The UUID of the class or None if not found.
        """
        result = await self.graphql_client.read_class_uuid(user_key)
        too_long = MultipleObjectsReturnedException(
            f"Found multiple classes with user_key = '{user_key}': {result}"
        )
        klass = only(result.objects, too_long=too_long)
        if klass is None:
            return None
        return klass.uuid

    async def load_mo_facet_uuid(self, user_key: str) -> UUID | None:
        """Find the UUID of a facet by user-key.

        Args:
            user_key: The user-key to lookup.

        Raises:
            MultipleObjectsReturnedException:
                If multiple facets share the same user-key.

        Returns:
            The uuid of the facet or None if not found.
        """
        result = await self.graphql_client.read_facet_uuid(user_key)
        too_long = MultipleObjectsReturnedException(
            f"Found multiple facets with user_key = '{user_key}': {result}"
        )
        facet = only(result.objects, too_long=too_long)
        if facet is None:
            return None
        return facet.uuid

    async def load_mo_it_user(
        self, uuid: UUID, current_objects_only=True
    ) -> ITUser | None:
        start = end = UNSET if current_objects_only else None
        results = await self.graphql_client.read_itusers([uuid], start, end)
        result = only(results.objects)
        if result is None:
            return None
        result_entry = extract_current_or_latest_validity(result.validities)
        if result_entry is None:
            return None
        entry = jsonable_encoder(result_entry)
        return ITUser.from_simplified_fields(
            user_key=entry["user_key"],
            itsystem_uuid=entry["itsystem_uuid"],
            from_date=entry["validity"]["from"],
            uuid=uuid,
            to_date=entry["validity"]["to"],
            person_uuid=entry["employee_uuid"],
            engagement_uuid=entry["engagement_uuid"],
        )

    async def load_mo_address(
        self, uuid: UUID, current_objects_only: bool = True
    ) -> Address | None:
        """
        Loads a mo address

        Notes
        ---------
        Only returns addresses which are valid today. Meaning the to/from date is valid.
        """
        logger.info("Loading address", uuid=uuid)

        start = end = UNSET if current_objects_only else None
        results = await self.graphql_client.read_addresses([uuid], start, end)
        result = only(results.objects)
        if result is None:
            return None
        result_entry = extract_current_or_latest_validity(result.validities)
        if result_entry is None:
            return None
        entry = jsonable_encoder(result_entry)
        address = Address.from_simplified_fields(
            value=entry["value"],
            address_type_uuid=entry["address_type"]["uuid"],
            from_date=entry["validity"]["from"],
            uuid=entry["uuid"],
            to_date=entry["validity"]["to"],
            value2=entry["value2"],
            person_uuid=entry["employee_uuid"],
            visibility_uuid=entry["visibility_uuid"],
            org_unit_uuid=entry["org_unit_uuid"],
            engagement_uuid=entry["engagement_uuid"],
        )

        return address

    # TODO: Offer this via a dataloader, and change calls to use that
    async def is_primaries(self, engagements: list[UUID]) -> list[bool]:
        engagements_set = set(engagements)
        result = await self.graphql_client.read_is_primary_engagements(
            list(engagements_set)
        )
        result_map = {
            obj.current.uuid: obj.current.is_primary
            for obj in result.objects
            if obj.current is not None
        }
        return [result_map.get(uuid, False) for uuid in engagements]

    async def load_mo_engagement(
        self,
        uuid: UUID,
        current_objects_only: bool = True,
    ) -> Engagement | None:
        start = end = UNSET if current_objects_only else None
        results = await self.graphql_client.read_engagements([uuid], start, end)
        result = only(results.objects)
        if result is None:
            return None
        result_entry = extract_current_or_latest_validity(result.validities)
        if result_entry is None:
            return None
        entry = jsonable_encoder(result_entry)
        engagement = Engagement.from_simplified_fields(
            org_unit_uuid=entry["org_unit_uuid"],
            person_uuid=entry["employee_uuid"],
            job_function_uuid=entry["job_function_uuid"],
            engagement_type_uuid=entry["engagement_type_uuid"],
            user_key=entry["user_key"],
            from_date=entry["validity"]["from"],
            to_date=entry["validity"]["to"],
            uuid=uuid,
            primary_uuid=entry["primary_uuid"],
            extension_1=entry["extension_1"],
            extension_2=entry["extension_2"],
            extension_3=entry["extension_3"],
            extension_4=entry["extension_4"],
            extension_5=entry["extension_5"],
            extension_6=entry["extension_6"],
            extension_7=entry["extension_7"],
            extension_8=entry["extension_8"],
            extension_9=entry["extension_9"],
            extension_10=entry["extension_10"],
        )
        return engagement

    async def load_mo_employee_addresses(
        self, employee_uuid: UUID, address_type_uuid: UUID
    ) -> list[Address]:
        """
        Loads all current addresses of a specific type for an employee
        """
        result = await self.graphql_client.read_employee_addresses(
            employee_uuid, address_type_uuid
        )
        # TODO: Bulk this
        address_uuids = [address.uuid for address in result.objects]
        output = await asyncio.gather(*map(self.load_mo_address, address_uuids))
        # If no active validities, pretend we did not get the object at all
        output = [obj for obj in output if obj is not None]
        return cast(list[Address], output)

    async def load_mo_org_unit_addresses(
        self, org_unit_uuid: OrgUnitUUID, address_type_uuid: UUID
    ) -> list[Address]:
        """
        Loads all current addresses of a specific type for an org unit
        """
        result = await self.graphql_client.read_org_unit_addresses(
            org_unit_uuid, address_type_uuid
        )
        # TODO: Bulk this
        address_uuids = [address.uuid for address in result.objects]
        output = await asyncio.gather(*map(self.load_mo_address, address_uuids))
        # If no active validities, pretend we did not get the object at all
        output = [obj for obj in output if obj is not None]
        return cast(list[Address], output)

    async def load_mo_employee_it_users(
        self,
        employee_uuid: UUID,
        it_system_uuid: UUID,
    ) -> list[ITUser]:
        """
        Load all current it users of a specific type linked to an employee
        """
        result = await self.graphql_client.read_ituser_by_employee_and_itsystem_uuid(
            employee_uuid, it_system_uuid
        )
        ituser_uuids = [ituser.uuid for ituser in result.objects]
        output = await asyncio.gather(*map(self.load_mo_it_user, ituser_uuids))
        # If no active validities, pretend we did not get the object at all
        output = [obj for obj in output if obj is not None]
        return cast(list[ITUser], output)

    async def load_mo_employee_engagement_dicts(
        self,
        employee_uuid: UUID,
        user_key: str | None = None,
    ) -> list[dict]:
        filter = EngagementFilter(employee=EmployeeFilter(uuids=[employee_uuid]))
        if user_key is not None:
            filter.user_keys = [user_key]

        result = await self.graphql_client.read_engagements_by_engagements_filter(
            filter
        )
        output = [
            jsonable_encoder(engagement.current)
            for engagement in result.objects
            if engagement.current
        ]
        return output

    async def load_mo_employee_engagements(
        self, employee_uuid: UUID
    ) -> list[Engagement]:
        """
        Load all current engagements linked to an employee
        """
        result = await self.graphql_client.read_engagements_by_employee_uuid(
            employee_uuid
        )
        engagement_uuids = [
            engagement.current.uuid
            for engagement in result.objects
            if engagement.current is not None
        ]
        output = await asyncio.gather(*map(self.load_mo_engagement, engagement_uuids))
        # If no active validities, pretend we did not get the object at all
        output = [obj for obj in output if obj is not None]
        return cast(list[Engagement], output)

    async def create_or_edit_mo_objects(self, objects: list[tuple[MOBase, Verb]]):
        def star(func):
            @wraps(func)
            def wrapper(tup: tuple) -> Any:
                return func(*tup)

            return wrapper

        def fix_verb(obj: MOBase, verb: Verb) -> tuple[MOBase, Verb]:
            if hasattr(obj, "terminate_"):
                return obj, Verb.TERMINATE
            return obj, verb

        # HACK to set termination verb, should be set within format_converted_objects instead,
        # but doing so requires restructuring the entire flow of the integration, which is a major
        # task best saved for later.
        objects = [fix_verb(obj, verb) for obj, verb in objects]

        # Split objects into groups
        verb_groups = bucket(objects, key=star(lambda _, verb: verb))
        creates = verb_groups[Verb.CREATE]
        edits = verb_groups[Verb.EDIT]
        terminates = verb_groups[Verb.TERMINATE]

        create_results, edit_results, terminate_results = await asyncio.gather(
            self.create([obj for obj, _ in creates]),
            self.edit([obj for obj, _ in edits]),
            self.terminate([obj for obj, _ in terminates]),
        )
        return cast(list[Any | None], create_results + edit_results + terminate_results)

    async def create_employee(self, obj: Employee) -> Any:
        model_client = self.context["legacy_model_client"]
        result = cast(list[Any], await model_client.upload([obj]))
        return one(result)

    async def create_address(self, obj: Address) -> Any:
        model_client = self.context["legacy_model_client"]
        result = cast(list[Any], await model_client.upload([obj]))
        return one(result)

    async def create_engagement(self, obj: Engagement) -> Any:
        model_client = self.context["legacy_model_client"]
        result = cast(list[Any], await model_client.upload([obj]))
        return one(result)

    async def create_ituser(self, obj: ITUser) -> Any:
        model_client = self.context["legacy_model_client"]
        result = cast(list[Any], await model_client.upload([obj]))
        return one(result)

    async def create_org_unit(self, obj: OrganisationUnit) -> Any:
        model_client = self.context["legacy_model_client"]
        result = cast(list[Any], await model_client.upload([obj]))
        return one(result)

    async def create_object(self, obj: MOBase) -> Any:
        match obj.type_:  # type: ignore
            case "address":
                assert isinstance(obj, Address)
                return await self.create_address(obj)
            case "employee":
                assert isinstance(obj, Employee)
                return await self.create_employee(obj)
            case "engagement":
                assert isinstance(obj, Engagement)
                return await self.create_engagement(obj)
            case "it":
                assert isinstance(obj, ITUser)
                return await self.create_ituser(obj)
            case other:
                raise NotImplementedError(f"Unable to create type: {other}")

    async def create(self, creates: list[MOBase]) -> list[Any]:
        tasks = [self.create_object(obj) for obj in creates]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        exceptions = cast(list[Exception], list(filter(is_exception, results)))
        if exceptions:
            raise ExceptionGroup("Exceptions during creation", exceptions)
        return results

    async def edit_employee(self, obj: Employee) -> Any:
        model_client = self.context["legacy_model_client"]
        result = cast(list[Any], await model_client.edit([obj]))
        return one(result)

    async def edit_address(self, obj: Address) -> Any:
        model_client = self.context["legacy_model_client"]
        result = cast(list[Any], await model_client.edit([obj]))
        return one(result)

    async def edit_engagement(self, obj: Engagement) -> Any:
        model_client = self.context["legacy_model_client"]
        result = cast(list[Any], await model_client.edit([obj]))
        return one(result)

    async def edit_ituser(self, obj: ITUser) -> Any:
        model_client = self.context["legacy_model_client"]
        result = cast(list[Any], await model_client.edit([obj]))
        return one(result)

    async def edit_object(self, obj: MOBase) -> Any:
        match obj.type_:  # type: ignore
            case "address":
                assert isinstance(obj, Address)
                return await self.edit_address(obj)
            case "employee":
                assert isinstance(obj, Employee)
                return await self.edit_employee(obj)
            case "engagement":
                assert isinstance(obj, Engagement)
                return await self.edit_engagement(obj)
            case "it":
                assert isinstance(obj, ITUser)
                return await self.edit_ituser(obj)
            case other:
                raise NotImplementedError(f"Unable to edit type: {other}")

    async def edit(self, edits: list[MOBase]) -> list[Any]:
        tasks = [self.edit_object(obj) for obj in edits]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        exceptions = cast(list[Exception], list(filter(is_exception, results)))
        if exceptions:
            raise ExceptionGroup("Exceptions during modification", exceptions)
        return results

    async def terminate_address(self, uuid: UUID, at: datetime) -> UUID:
        result = await self.graphql_client.address_terminate(
            AddressTerminateInput(uuid=uuid, to=at)
        )
        return result.uuid

    async def terminate_engagement(self, uuid: UUID, at: datetime) -> UUID:
        result = await self.graphql_client.engagement_terminate(
            EngagementTerminateInput(uuid=uuid, to=at)
        )
        return result.uuid

    async def terminate_ituser(self, uuid: UUID, at: datetime) -> UUID:
        result = await self.graphql_client.ituser_terminate(
            ITUserTerminateInput(uuid=uuid, to=at)
        )
        return result.uuid

    async def terminate_object(self, uuid: UUID, at: datetime, motype: str) -> UUID:
        """Terminate a detail.

        This method calls the appropriate `terminate_x` method to terminate the object.

        Args:
            terminatee: The detail to terminate

        Returns:
            UUID of the terminated entry
        """

        match motype:
            case "address":
                return await self.terminate_address(uuid, at)
            case "engagement":
                return await self.terminate_engagement(uuid, at)
            case "it":
                return await self.terminate_ituser(uuid, at)
            case _:
                raise NotImplementedError(f"Unable to terminate type: {motype}")

    async def terminate(self, terminatees: list[Any]) -> list[UUID]:
        """Terminate a list of details.

        This method calls `terminate_object` for each objects in parallel.

        Args:
            terminatees: The list of details to terminate.

        Returns:
            UUIDs of the terminated entries
        """
        detail_terminations: list[dict[str, Any]] = [
            {
                "motype": terminate.type_,
                "uuid": terminate.uuid,
                "at": terminate.terminate_,
            }
            for terminate in terminatees
        ]
        tasks = [self.terminate_object(**detail) for detail in detail_terminations]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        exceptions = cast(list[Exception], list(filter(is_exception, results)))
        if exceptions:
            raise ExceptionGroup("Exceptions during termination", exceptions)
        return cast(list[UUID], results)

    async def create_mo_class(
        self,
        name: str,
        user_key: str,
        facet_uuid: UUID,
        scope: str | None = None,
    ) -> UUID:
        """Creates a class in MO.

        Args:
            name: The name for the class.
            user_key: The user-key for the class.
            facet_uuid: The UUID of the facet to attach this class to.
            scope: The optional scope to assign to the class.

        Returns:
            The uuid of the existing or newly created class.
        """
        async with self.create_mo_class_lock:
            # If class already exists, noop
            uuid = await self.load_mo_class_uuid(user_key)
            if uuid:
                logger.info("MO class exists", user_key=user_key)
                return uuid

            logger.info("Creating MO class", user_key=user_key)
            input = ClassCreateInput(
                name=name,
                user_key=user_key,
                facet_uuid=facet_uuid,
                scope=scope,
                validity=RAOpenValidityInput(from_=None),
            )
            result = await self.graphql_client.class_create(input)
            return result.uuid
