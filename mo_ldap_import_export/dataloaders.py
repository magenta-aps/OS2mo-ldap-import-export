# SPDX-FileCopyrightText: 2019-2020 Magenta ApS
# SPDX-License-Identifier: MPL-2.0
"""Dataloaders to bulk requests."""
import asyncio
from contextlib import suppress
from datetime import datetime
from enum import auto
from enum import Enum
from functools import partialmethod
from functools import wraps
from typing import Any
from typing import cast
from typing import Literal
from uuid import UUID

import structlog
from fastapi.encoders import jsonable_encoder
from gql.client import AsyncClientSession
from graphql import DocumentNode
from ldap3 import BASE
from ldap3.core.exceptions import LDAPInvalidValueError
from ldap3.protocol import oid
from ldap3.utils.dn import safe_dn
from ldap3.utils.dn import to_dn
from more_itertools import bucket
from more_itertools import one
from more_itertools import only
from more_itertools import partition
from ramodels.mo import MOBase
from ramodels.mo._shared import EngagementRef
from ramodels.mo._shared import validate_cpr
from ramodels.mo.details.address import Address
from ramodels.mo.details.engagement import Engagement
from ramodels.mo.details.it_system import ITUser
from ramodels.mo.employee import Employee

from .autogenerated_graphql_client import GraphQLClient
from .autogenerated_graphql_client.base_model import UNSET
from .autogenerated_graphql_client.input_types import AddressTerminateInput
from .autogenerated_graphql_client.input_types import ClassCreateInput
from .autogenerated_graphql_client.input_types import ClassUpdateInput
from .autogenerated_graphql_client.input_types import EmployeeFilter
from .autogenerated_graphql_client.input_types import EngagementFilter
from .autogenerated_graphql_client.input_types import EngagementTerminateInput
from .autogenerated_graphql_client.input_types import ITSystemCreateInput
from .autogenerated_graphql_client.input_types import ITUserTerminateInput
from .autogenerated_graphql_client.input_types import RAOpenValidityInput
from .config import Settings
from .environments import filter_remove_curly_brackets
from .exceptions import AttributeNotFound
from .exceptions import DNNotFound
from .exceptions import InvalidChangeDict
from .exceptions import MultipleObjectsReturnedException
from .exceptions import NoObjectsReturnedException
from .exceptions import NotEnabledException
from .exceptions import UUIDNotFoundException
from .ldap import get_attribute_types
from .ldap import get_ldap_attributes
from .ldap import get_ldap_object
from .ldap import get_ldap_schema
from .ldap import get_ldap_superiors
from .ldap import is_uuid
from .ldap import ldap_compare
from .ldap import ldap_modify
from .ldap import ldap_modify_dn
from .ldap import make_ldap_object
from .ldap import object_search
from .ldap import paged_search
from .ldap import single_object_search
from .ldap_classes import LdapObject
from .types import CPRNumber
from .types import DN
from .types import OrgUnitUUID
from .usernames import UserNameGenerator
from .utils import combine_dn_strings
from .utils import extract_cn_from_dn
from .utils import extract_ou_from_dn
from .utils import mo_datestring_to_utc
from .utils import remove_cn_from_dn

logger = structlog.stdlib.get_logger()


class Verb(Enum):
    CREATE = auto()
    EDIT = auto()
    TERMINATE = auto()


class DataLoader:
    def __init__(self, context):
        self.context = context
        self.user_context = context["user_context"]
        self.ldap_connection = self.user_context["ldap_connection"]
        self.attribute_types = get_attribute_types(self.ldap_connection)
        self.single_value = {k: v.single_value for k, v in self.attribute_types.items()}
        self._mo_to_ldap_attributes = []
        self.create_mo_class_lock = asyncio.Lock()

        # Relate graphQL object types (left) to AMQP routing key object types (right)
        self.object_type_dict = {
            "employees": "person",
            "org_units": "org_unit",
            "addresses": "address",
            "itusers": "ituser",
            "engagements": "engagement",
        }

        self.object_type_dict_inv = {
            str(v): k for k, v in self.object_type_dict.items()
        }

        self.supported_object_types = list(self.object_type_dict_inv.keys())

    def _check_if_empty(self, result: dict):
        for key, value in result.items():
            if "objects" in value and len(value["objects"]) == 0:
                raise NoObjectsReturnedException(
                    f"query_result['{key}'] is empty. "
                    f"Does the '{key}' object still exist as a current object? "
                    f"Does the '{key}' object exist in MO?"
                )

    @property
    def graphql_client(self) -> GraphQLClient:
        return cast(GraphQLClient, self.context["graphql_client"])

    @property
    def sync_tool(self):
        from .import_export import SyncTool

        return cast(SyncTool, self.user_context["sync_tool"])

    @property
    def mo_to_ldap_attributes(self):
        """
        Populates self._mo_to_ldap_attributes and returns it.

        self._mo_to_ldap_attributes is a list of all LDAP attribute names which
        are synchronized to LDAP

        Notes
        -------
        This is not done in __init__() because the converter is not initialized yet,
        when we initialize the dataloader.
        """
        if not self._mo_to_ldap_attributes:
            converter = self.user_context["converter"]
            for json_dict in converter.mapping["mo_to_ldap"].values():
                self._mo_to_ldap_attributes.extend(list(json_dict.keys()))
        return self._mo_to_ldap_attributes

    def shared_attribute(self, attribute: str):
        """
        Determine if an attribute is shared between multiple LDAP objects.

        Parameters
        ------------
        attribute : str
            LDAP attribute name

        Returns
        ----------
        return_value : bool
            True if the attribute is shared between different LDAP objects, False if it
            is not.

        Examples
        -----------
        >>> self.shared_attribute("cpr_no")
        >>> True

        The "cpr_no" attribute is generally shared between different LDAP objects.
        Therefore the return value is "True"

        >>> self.shared_attribute("mobile_phone_no")
        >>> False

        An attribute which contains a phone number is generally only used by a single
        LDAP object. Therefore the return value is "False"

        Notes
        -------
        The return value in above examples depends on the json dictionary.
        """
        occurences = self.mo_to_ldap_attributes.count(attribute)
        if occurences == 1:
            return False
        elif occurences > 1:
            return True
        else:
            raise AttributeNotFound(
                f"'{attribute}' not found in 'mo_to_ldap' attributes"
            )

    async def query_mo(
        self, query: DocumentNode, raise_if_empty: bool = True, variable_values={}
    ):
        graphql_session: AsyncClientSession = self.context["legacy_graphql_session"]
        result = await graphql_session.execute(
            query, variable_values=jsonable_encoder(variable_values)
        )
        if raise_if_empty:
            self._check_if_empty(result)
        return result

    async def query_mo_paged(self, query):
        result = await self.query_mo(query, raise_if_empty=False)

        for key in result.keys():
            cursor = result[key]["page_info"]["next_cursor"]
            page_counter = 0

            while cursor:
                logger.info("Loading next page", key=key, page=page_counter)
                next_result = await self.query_mo(
                    query,
                    raise_if_empty=False,
                    variable_values={"cursor": cursor},
                )

                # Append next page to result
                result[key]["objects"] += next_result[key]["objects"]

                # Update cursor and page counter
                page_counter += 1
                cursor = next_result[key]["page_info"]["next_cursor"]

        return result

    async def load_ldap_object(
        self,
        dn: DN,
        attributes: list | None,
        nest: bool = True,
        run_discriminator: bool = True,
    ) -> LdapObject:  # pragma: no cover
        # TODO: Actually eliminate this function by calling get_ldap_object directly.
        #       Be warned though, doing so breaks ~25 tests because of bad mocking.
        return await get_ldap_object(
            dn, self.context, nest, attributes, run_discriminator
        )

    async def load_ldap_cpr_object(
        self,
        cpr_no: CPRNumber,
        json_key: str,
        additional_attributes: list[str] = [],
    ) -> list[LdapObject]:
        """
        Loads an ldap object which can be found using a cpr number lookup

        Accepted json_keys are:
            - 'Employee'
            - a MO address type name
        """
        try:
            validate_cpr(cpr_no)
        except (ValueError, TypeError):
            raise NoObjectsReturnedException(f"cpr_no '{cpr_no}' is invalid")

        cpr_field = self.user_context["cpr_field"]
        if not cpr_field:
            raise NoObjectsReturnedException("cpr_field is not configured")

        settings = self.user_context["settings"]

        search_base = settings.ldap_search_base
        ous_to_search_in = settings.ldap_ous_to_search_in
        search_bases = [
            combine_dn_strings([ou, search_base]) for ou in ous_to_search_in
        ]
        converter = self.user_context["converter"]

        object_class = converter.find_ldap_object_class(json_key)
        attributes = converter.get_ldap_attributes(json_key) + additional_attributes

        object_class_filter = f"objectclass={object_class}"
        cpr_filter = f"{cpr_field}={cpr_no}"

        searchParameters = {
            "search_base": search_bases,
            "search_filter": f"(&({object_class_filter})({cpr_filter}))",
            "attributes": list(set(attributes)),
        }
        ldap_connection = self.context["user_context"]["ldap_connection"]
        search_results = await object_search(searchParameters, ldap_connection)
        # TODO: Asyncio gather this
        ldap_objects: list[LdapObject] = [
            await make_ldap_object(search_result, self.context)
            for search_result in search_results
        ]
        dns = [obj.dn for obj in ldap_objects]
        logger.info("Found LDAP(s) object", dns=dns)
        return ldap_objects

    def ou_in_ous_to_write_to(self, dn: str) -> bool:
        """
        Determine if an OU is among those to which we are allowed to write.
        """
        settings = self.user_context["settings"]

        if "" in settings.ldap_ous_to_write_to:
            # Empty string means that it is allowed to write to all OUs
            return True

        ou = extract_ou_from_dn(dn)
        ous_to_write_to = [safe_dn(ou) for ou in settings.ldap_ous_to_write_to]
        for ou_to_write_to in ous_to_write_to:
            if ou.endswith(ou_to_write_to):
                # If an OU ends with one of the OUs-to-write-to, it's OK.
                # For example, if we are only allowed to write to "OU=foo",
                # Then we are also allowed to write to "OU=bar,OU=foo", which is a
                # sub-OU inside "OU=foo"
                return True

        logger.info("OU not in OUs to write", ou=ou, ous_to_write_to=ous_to_write_to)
        return False

    async def modify_ldap(
        self,
        operation: Literal[
            "MODIFY_ADD", "MODIFY_DELETE", "MODIFY_REPLACE", "MODIFY_INCREMENT"
        ],
        dn: str,
        attribute: str,
        value: list[str] | str,
    ) -> dict | None:
        """
        Modifies LDAP and adds the dn to dns_to_ignore
        """
        # TODO: Remove this when ldap3s read-only flag works
        settings = self.user_context["settings"]
        if settings.ldap_read_only:
            logger.info(
                "LDAP connection is read-only",
                operation="modify_ldap",
                dn=dn,
                attribute=attribute,
            )
            raise NotEnabledException("LDAP connection is read-only")

        # Checks
        if not self.ou_in_ous_to_write_to(dn):
            return None

        if isinstance(value, list):
            value = only(
                value,
                default="",
                too_long=InvalidChangeDict(
                    "Exactly one value can be changed at a time"
                ),
            )

        # Compare to LDAP
        value_exists = ldap_compare(self.ldap_connection, dn, attribute, value)

        # If the value is already as expected, and we are not deleting, we are done
        if value_exists and "DELETE" not in operation:
            logger.info(
                "Attribute value already exists",
                attribute=attribute,
                value_to_modify=value,
            )
            return None

        # Modify LDAP
        changes = {attribute: [(operation, value)]}
        logger.info("Uploading the changes", changes=changes, dn=dn)
        _, result = ldap_modify(self.ldap_connection, dn, changes)
        logger.info("LDAP Result", result=result, dn=dn)

        # If successful, the importer should ignore this DN
        if result["description"] == "success":
            # Clean all old entries
            self.sync_tool.dns_to_ignore.clean()

            # Only add if nothing is there yet. Otherwise we risk adding an
            # ignore-command for every modified parameter
            #
            # Also: even if an LDAP attribute gets modified by us twice within a
            # couple of seconds, it should still only be ignored once; Because we
            # only retrieve the latest state of the LDAP object when polling
            if not self.sync_tool.dns_to_ignore[dn]:
                self.sync_tool.dns_to_ignore.add(dn)

        return result

    add_ldap = partialmethod(modify_ldap, "MODIFY_ADD")
    delete_ldap = partialmethod(modify_ldap, "MODIFY_DELETE")
    replace_ldap = partialmethod(modify_ldap, "MODIFY_REPLACE")

    async def load_ldap_OUs(self, search_base: str | None = None) -> dict:
        """
        Returns a dictionary where the keys are OU strings and the items are dicts
        which contain information about the OU
        """
        searchParameters: dict = {
            "search_filter": "(objectclass=OrganizationalUnit)",
            "attributes": [],
        }

        responses = await paged_search(
            self.context,
            searchParameters,
            search_base=search_base,
            mute=True,
        )

        dns = [r["dn"] for r in responses]
        output = {}

        for dn in dns:
            searchParameters = {
                "search_filter": "(objectclass=user)",
                "attributes": [],
                "size_limit": 1,
            }

            responses = await paged_search(
                self.context,
                searchParameters,
                search_base=dn,
                mute=True,
            )
            ou = extract_ou_from_dn(dn)
            if len(responses) == 0:
                output[ou] = {"empty": True}
            else:
                output[ou] = {"empty": False}
            output[ou]["dn"] = dn

        return output

    async def add_ldap_object(self, dn: str, attributes: dict[str, Any] | None = None):
        """
        Adds a new object to LDAP

        Parameters
        ---------------
        attributes : dict
            dictionary with attributes to populate in LDAP, when creating the user.
            See https://ldap3.readthedocs.io/en/latest/add.html for more information

        """
        settings: Settings = self.user_context["settings"]
        # TODO: Remove this when ldap3s read-only flag works
        if settings.ldap_read_only:
            logger.info(
                "LDAP connection is read-only",
                operation="add_ldap_object",
                dn=dn,
                attributes=attributes,
            )
            raise NotEnabledException("LDAP connection is read-only")

        if not settings.add_objects_to_ldap:
            logger.info(
                "Adding LDAP objects is disabled",
                operation="add_ldap_object",
                dn=dn,
                attributes=attributes,
            )
            raise NotEnabledException("Adding LDAP objects is disabled")

        if not self.ou_in_ous_to_write_to(dn):
            return

        logger.info("Adding user to LDAP", dn=dn, attributes=attributes)
        self.ldap_connection.add(
            dn,
            self.user_context["converter"].find_ldap_object_class("Employee"),
            attributes=attributes,
        )
        result: dict = self.ldap_connection.result
        logger.info("LDAP Result", result=result, dn=dn)

    @staticmethod
    def decompose_ou_string(ou: str) -> list[str]:
        """
        Decomposes an OU string and returns a list of OUs where the first one is the
        given OU string, and the last one if the highest parent OU

        Example
        -----------
        >>> ou = 'OU=foo,OU=bar'
        >>> decompose_ou_string(ou)
        >>> ['OU=foo,OU=bar', 'OU=bar']
        """

        ou_parts = to_dn(ou)
        output = []
        for i in range(len(ou_parts)):
            output.append(combine_dn_strings(ou_parts[i:]))

        return output

    async def create_ou(self, ou: str) -> None:
        """
        Creates an OU. If the parent OU does not exist, creates that one first
        """
        settings = self.user_context["settings"]

        # TODO: Remove this when ldap3s read-only flag works
        if settings.ldap_read_only:
            logger.info("LDAP connection is read-only", operation="create_ou", ou=ou)
            raise NotEnabledException("LDAP connection is read-only")

        if not settings.add_objects_to_ldap:
            logger.info("Adding LDAP objects is disabled", operation="create_ou", ou=ou)
            raise NotEnabledException("Adding LDAP objects is disabled")

        if not self.ou_in_ous_to_write_to(ou):
            return

        # TODO: Search for specific OUs as needed instead of reading all of LDAP?
        ou_dict = await self.load_ldap_OUs()

        # Create OUs top-down (unless they already exist)
        for ou_to_create in self.decompose_ou_string(ou)[::-1]:
            if ou_to_create not in ou_dict:
                logger.info("Creating OU", ou_to_create=ou_to_create)
                dn = combine_dn_strings([ou_to_create, settings.ldap_search_base])

                self.ldap_connection.add(dn, "OrganizationalUnit")
                result: dict = self.ldap_connection.result
                logger.info("LDAP Result", result=result, dn=dn)

    async def delete_ou(self, ou: str) -> None:
        """
        Deletes an OU. If the parent OU is empty after deleting, also deletes that one

        Notes
        --------
        Only deletes OUs which are empty
        """
        settings = self.user_context["settings"]
        # TODO: Remove this when ldap3s read-only flag works
        if settings.ldap_read_only:
            logger.info("LDAP connection is read-only", operation="delete_ou", ou=ou)
            raise NotEnabledException("LDAP connection is read-only")

        if not self.ou_in_ous_to_write_to(ou):
            return

        for ou_to_delete in self.decompose_ou_string(ou):
            # TODO: Search for specific OUs as needed instead of reading all of LDAP?
            ou_dict = await self.load_ldap_OUs()
            if (
                ou_dict.get(ou_to_delete, {}).get("empty", False)
                and ou_to_delete != settings.ldap_ou_for_new_users
            ):
                logger.info("Deleting OU", ou_to_delete=ou_to_delete)
                dn = combine_dn_strings([ou_to_delete, settings.ldap_search_base])
                self.ldap_connection.delete(dn)
                response: dict = self.ldap_connection.result
                logger.info("LDAP Result", result=response, dn=dn)

    async def move_ldap_object(self, old_dn: str, new_dn: str) -> bool:
        """
        Moves an LDAP object from one DN to another. Returns True if the move was
        successful.
        """
        settings = self.user_context["settings"]

        # TODO: Remove this when ldap3s read-only flag works
        if settings.ldap_read_only:
            logger.info(
                "LDAP connection is read-only",
                operation="move_ldap_object",
                old_dn=old_dn,
                new_dn=new_dn,
            )
            raise NotEnabledException("LDAP connection is read-only")

        if not settings.add_objects_to_ldap:
            logger.info(
                "Adding LDAP objects is disabled",
                operation="move_ldap_object",
                old_dn=old_dn,
                new_dn=new_dn,
            )
            raise NotEnabledException("Adding LDAP objects is disabled")

        if not self.ou_in_ous_to_write_to(new_dn):
            return False

        logger.info("Moving entry", old_dn=old_dn, new_dn=new_dn)

        _, result = ldap_modify_dn(
            self.ldap_connection,
            old_dn,
            extract_cn_from_dn(new_dn),
            new_superior=remove_cn_from_dn(new_dn),
        )
        logger.info("LDAP Result", result=result, new_dn=new_dn, old_dn=old_dn)
        return True if result["description"] == "success" else False

    async def modify_ldap_object(
        self,
        object_to_modify: LdapObject,
        json_key: str,
        overwrite: bool = False,
        delete: bool = False,
    ) -> list[dict]:
        """
        Parameters
        -------------
        object_to_modify : LDAPObject
            object to upload to LDAP
        json_key : str
            json key to upload. e.g. 'Employee' or 'Engagement' or another key present
            in the json dictionary.
        overwrite: bool
            Set to True to overwrite contents in LDAP
        delete: bool
            Set to True to delete contents in LDAP, instead of creating/modifying them
        """
        converter = self.user_context["converter"]
        if not converter._export_to_ldap_(json_key):
            logger.info("_export_to_ldap_ == False.", json_key=json_key)
            return []
        success = 0
        failed = 0

        parameters_to_modify = list(object_to_modify.dict().keys())

        logger.info("Uploading object", object_to_modify=object_to_modify)
        parameters_to_modify = [p for p in parameters_to_modify if p != "dn"]
        dn = object_to_modify.dn
        results = []

        if delete:
            # Only delete parameters which are not shared between different objects.
            # For example: 'org-unit name' should not be deleted if both
            # engagements and org unit addresses use it;
            #
            # If we would delete 'org-unit name' as a part of an org-unit address delete
            # operation, We would suddenly not be able to import engagements any more.

            # TODO: This means that any attribute referenced by multiple templates will
            #       literally never be cleared. This seems potentially highly problematic?
            #       We should probably consider if this is the optimal design.
            #
            #       I have discussed this with Casper, and the solution seems to be that
            #       we have to synchronize the entire state at once, otherwise we cannot
            #       possibly ensure deletions work as expected. This is also what the
            #       Omada integration does.
            parameters_to_modify = [
                p for p in parameters_to_modify if not self.shared_attribute(p)
            ]

        for parameter_to_modify in parameters_to_modify:
            value = getattr(object_to_modify, parameter_to_modify)
            value_to_modify: list[str] = [] if value is None else [value]

            operation = None
            if delete:
                operation = self.delete_ldap
            elif self.single_value[parameter_to_modify] or overwrite:
                operation = self.replace_ldap
            else:
                operation = self.add_ldap

            try:
                response = await operation(dn, parameter_to_modify, value_to_modify)
            except LDAPInvalidValueError:
                logger.warning("LDAPInvalidValueError exception", exc_info=True)
                failed += 1
                continue

            if response and response["description"] == "success":
                success += 1
            elif response:
                failed += 1

            if response:
                results.append(response)

        logger.info(
            "Succeeded/failed MODIFY_* operations",
            success=success,
            failed=failed,
        )

        return results

    def make_overview_entry(self, attributes, superiors, example_value_dict=None):
        attribute_dict = {}
        for attribute in attributes:
            # skip unmapped types
            if attribute not in self.attribute_types:
                continue
            syntax = self.attribute_types[attribute].syntax

            # decoded syntax tuple structure: (oid, kind, name, docs)
            syntax_decoded = oid.decode_syntax(syntax)
            details_dict = {
                "single_value": self.attribute_types[attribute].single_value,
                "syntax": syntax,
            }
            if syntax_decoded:
                details_dict["field_type"] = syntax_decoded[2]

            if example_value_dict:
                if attribute in example_value_dict:
                    details_dict["example_value"] = example_value_dict[attribute]

            attribute_dict[attribute] = details_dict

        return {
            "superiors": superiors,
            "attributes": attribute_dict,
        }

    def load_ldap_overview(self):
        schema = get_ldap_schema(self.ldap_connection)

        all_object_classes = sorted(list(schema.object_classes.keys()))

        output = {}
        for ldap_class in all_object_classes:
            all_attributes = get_ldap_attributes(self.ldap_connection, ldap_class)
            superiors = get_ldap_superiors(self.ldap_connection, ldap_class)
            output[ldap_class] = self.make_overview_entry(all_attributes, superiors)

        return output

    async def find_mo_employee_uuid_via_cpr_number(self, dn: str) -> set[UUID]:
        cpr_field = self.user_context["cpr_field"]
        if cpr_field is None:
            return set()

        ldap_object = await self.load_ldap_object(
            dn, [cpr_field], run_discriminator=False
        )
        # Try to get the cpr number from LDAP and use that.
        try:
            raw_cpr_no = getattr(ldap_object, cpr_field)
            # NOTE: Not sure if this only necessary for the mocked server or not
            if isinstance(raw_cpr_no, list):
                raw_cpr_no = one(raw_cpr_no)
            cpr_no = validate_cpr(str(raw_cpr_no))
            assert cpr_no is not None
            cpr_number = CPRNumber(cpr_no)
        except ValueError:
            return set()

        result = await self.graphql_client.read_employee_uuid_by_cpr_number(cpr_number)
        return {employee.uuid for employee in result.objects}

    async def find_mo_employee_uuid_via_ituser(self, dn: str) -> set[UUID]:
        unique_uuid = await self.get_ldap_unique_ldap_uuid(dn)
        result = await self.graphql_client.read_employee_uuid_by_ituser_user_key(
            str(unique_uuid)
        )
        return {
            ituser.current.employee_uuid
            for ituser in result.objects
            if ituser.current is not None and ituser.current.employee_uuid is not None
        }

    async def find_mo_employee_uuid(self, dn: str) -> UUID | None:
        cpr_results = await self.find_mo_employee_uuid_via_cpr_number(dn)
        if len(cpr_results) == 1:
            uuid = one(cpr_results)
            logger.info("Found employee via CPR matching", dn=dn, uuid=uuid)
            return uuid

        ituser_results = await self.find_mo_employee_uuid_via_ituser(dn)
        if len(ituser_results) == 1:
            uuid = one(ituser_results)
            logger.info("Found employee via ITUser matching", dn=dn, uuid=uuid)
            return uuid

        # TODO: Return an ExceptionGroup with both
        # NOTE: This may break a lot of things, because we explicitly match against MultipleObjectsReturnedException
        if len(cpr_results) > 1:
            raise MultipleObjectsReturnedException(f"Multiple CPR matches for dn={dn}")

        if len(ituser_results) > 1:
            raise MultipleObjectsReturnedException(
                f"Multiple ITUser matches for dn={dn}"
            )

        logger.info("No matching employee", dn=dn)
        return None

    async def find_mo_engagement_uuid(self, dn: DN) -> None | UUID:
        # Get Unique LDAP UUID from DN, then get engagement by looking for IT user with that
        # Unique LDAP UUID in MO.

        settings = self.user_context["settings"]
        ldap_object = await self.load_ldap_object(
            dn, [settings.ldap_unique_id_field], run_discriminator=False
        )
        raw_unique_uuid = getattr(ldap_object, settings.ldap_unique_id_field)
        # NOTE: Not sure if this only necessary for the mocked server or not
        if isinstance(raw_unique_uuid, list):
            raw_unique_uuid = one(raw_unique_uuid)
        unique_uuid = filter_remove_curly_brackets(raw_unique_uuid)

        itsystem_uuid = self.get_ldap_it_system_uuid()
        if itsystem_uuid is None:
            logger.info(
                "Could not find engagement UUID for DN",
                dn=dn,
                unique_ldap_uuid=unique_uuid,
                itsystem_uuid=itsystem_uuid,
            )
            return None

        result = await self.graphql_client.read_engagement_uuid_by_ituser_user_key(
            unique_uuid, UUID(itsystem_uuid)
        )
        engagement_uuids = {
            ituser.current.engagement_uuid
            for ituser in result.objects
            if ituser.current is not None
        }
        engagement_uuid = only(engagement_uuids)
        if engagement_uuid is None:
            logger.info(
                "Could not find engagement UUID for DN",
                dn=dn,
                unique_ldap_uuid=unique_uuid,
                itsystem_uuid=itsystem_uuid,
                engagement_uuid=engagement_uuid,
            )
            return None

        return engagement_uuid

    def get_ldap_it_system_uuid(self) -> str | None:
        """
        Return the IT system uuid belonging to the LDAP-it-system
        Return None if the LDAP-it-system is not found.
        """
        converter = self.user_context["converter"]
        user_key = self.user_context["ldap_it_system_user_key"]
        try:
            return cast(str, converter.get_it_system_uuid(user_key))
        except UUIDNotFoundException:
            logger.info(
                "UUID Not found",
                suggestion=f"Does the '{user_key}' it-system exist?",
            )
            return None

    async def get_ldap_dn(self, unique_ldap_uuid: UUID) -> DN:
        """
        Given an unique_ldap_uuid, find the DistinguishedName
        """
        logger.info("Looking for LDAP object", unique_ldap_uuid=unique_ldap_uuid)
        searchParameters = {
            "search_base": f"<GUID={unique_ldap_uuid}>",
            "search_filter": "(objectclass=*)",
            "attributes": [],
            "search_scope": BASE,
        }

        search_result = await single_object_search(searchParameters, self.context)
        dn: str = search_result["dn"]
        return dn

    async def get_ldap_unique_ldap_uuid(self, dn: str) -> UUID:
        """
        Given a DN, find the unique_ldap_uuid
        """
        settings = self.user_context["settings"]
        logger.info("Looking for LDAP object", dn=dn)
        ldap_object = await self.load_ldap_object(dn, [settings.ldap_unique_id_field])
        uuid = getattr(ldap_object, settings.ldap_unique_id_field)
        if not uuid:
            # Some computer-account objects has no samaccountname
            raise NoObjectsReturnedException(
                f"Object has no {settings.ldap_unique_id_field}"
            )
        return UUID(uuid)

    def extract_unique_ldap_uuids(self, it_users: list[ITUser]) -> set[UUID]:
        """
        Extracts unique ldap uuids from a list of it-users
        """
        it_user_keys = {ituser.user_key for ituser in it_users}
        not_uuids, uuids = partition(is_uuid, it_user_keys)
        for user_key in not_uuids:
            logger.info(
                "IT-user is not a UUID",
                user_key=user_key,
            )
        # TODO: Check for duplicates?
        return set(map(UUID, uuids))

    async def extract_unique_dns(self, it_users: list[ITUser]) -> set[DN]:
        unique_uuids = self.extract_unique_ldap_uuids(it_users)
        # TODO: DataLoader / bulk here instead of this
        dns = await asyncio.gather(*[self.get_ldap_dn(uuid) for uuid in unique_uuids])
        return set(dns)

    async def find_mo_employee_dn_by_itsystem(self, uuid: UUID) -> set[DN]:
        """Tries to find the LDAP DNs belonging to a MO employee via ITUsers.

        Args:
            uuid: UUID of the employee to try to find DNs for.

        Returns:
            A potentially empty set of DNs.
        """
        # TODO: How do we know if the ITUser is up-to-date with the newest DNs in AD?

        # The ITSystem only exists if configured to do so
        raw_it_system_uuid = self.get_ldap_it_system_uuid()
        # If it does not exist, we cannot fetch users for it
        if raw_it_system_uuid is None:
            return set()

        it_system_uuid = UUID(raw_it_system_uuid)
        try:
            it_users = await self.load_mo_employee_it_users(uuid, it_system_uuid)
        except NoObjectsReturnedException:  # pragma: no cover
            return set()
        dns = await self.extract_unique_dns(it_users)
        # No DNs, no problem
        if not dns:
            return set()

        # If we have one or more ITUsers (with valid dns), return those
        logger.info(
            "Found DN(s) using ITUser lookup",
            dns=dns,
            employee_uuid=uuid,
        )
        return dns

    async def find_mo_employee_dn_by_cpr_number(self, uuid: UUID) -> set[DN]:
        """Tries to find the LDAP DNs belonging to a MO employee via CPR numbers.

        Args:
            uuid: UUID of the employee to try to find DNs for.

        Returns:
            A potentially empty set of DNs.
        """
        # If the employee has a cpr-no, try using that to find matchind DNs
        employee = await self.load_mo_employee(uuid)
        cpr_no = CPRNumber(employee.cpr_no) if employee.cpr_no else None
        # No CPR, no problem
        if not cpr_no:
            return set()

        logger.info(
            "Attempting CPR number lookup",
            employee_uuid=uuid,
        )
        try:
            dns = {
                obj.dn for obj in await self.load_ldap_cpr_object(cpr_no, "Employee")
            }
        except NoObjectsReturnedException:
            return set()
        if not dns:
            return set()
        logger.info(
            "Found DN(s) using CPR number lookup",
            dns=dns,
            employee_uuid=uuid,
        )
        return dns

    async def find_mo_employee_dn(self, uuid: UUID) -> set[DN]:
        """Tries to find the LDAP DNs belonging to a MO employee.

        Args:
            uuid: UUID of the employee to try to find DNs for.

        Returns:
            A potentially empty set of DNs.
        """
        # TODO: This should probably return a list of EntityUUIDs rather than DNs
        #       However this should probably be a change away from DNs in general
        logger.info(
            "Attempting to find DNs",
            employee_uuid=uuid,
        )
        # TODO: We should be able to trust just the ITUsers, however we do not.
        #       Maybe once the code becomes easier to reason about, we can get to that.
        #       But for now, we fetch all accounts, and use the discriminator.
        #
        # TODO: We may want to expand this in the future to also check for half-created
        #       objects, to support scenarios where the application may crash after
        #       creating an LDAP account, but before making a MO ITUser.
        ituser_dns, cpr_number_dns = await asyncio.gather(
            self.find_mo_employee_dn_by_itsystem(uuid),
            self.find_mo_employee_dn_by_cpr_number(uuid),
        )
        dns = ituser_dns | cpr_number_dns
        if dns:
            return dns
        logger.warning(
            "Unable to find DNs for MO employee",
            employee_uuid=uuid,
        )
        return set()

    async def find_or_make_mo_employee_dn(self, uuid: UUID) -> set[DN]:
        """Finds or creates an LDAP DNs beloning to a MO employee.

        Note:
            If a DN(s) is found, one will not be created.
            If a DN(s) is not found, one will be created.

        Args:
            uuid: UUID of the employee to try to find DNs for.

        Raises:
            DNNotFound: If no DN(s) was found, and we cannot create one.

        Returns:
            A potentially empty set of DNs.
        """
        logger.info(
            "Attempting to find DN",
            employee_uuid=uuid,
        )
        dns = await self.find_mo_employee_dn(uuid)
        if dns:
            return dns
        return {await self.make_mo_employee_dn(uuid)}

    async def make_mo_employee_dn(self, uuid: UUID) -> DN:
        raw_it_system_uuid = self.get_ldap_it_system_uuid()
        employee = await self.load_mo_employee(uuid)
        cpr_no = CPRNumber(employee.cpr_no) if employee.cpr_no else None

        # Check if we even dare create a DN
        if raw_it_system_uuid is None and cpr_no is None:
            logger.warning(
                "Could not or generate a DN for employee (cannot correlate)",
                employee_uuid=uuid,
            )
            raise DNNotFound("Unable to generate DN, no correlation key available")

        # If we did not find a DN neither via ITUser nor via CPR-number, then we want
        # to create one, by generating a DN, importing the user and potentially creating
        # a binding between the two.
        username_generator: UserNameGenerator = self.user_context["username_generator"]

        logger.info(
            "Generating DN for user",
            employee_uuid=uuid,
        )
        # NOTE: This not only generates the DN as the name suggests,
        #       rather it also *creates it in LDAP*, be warned!
        #
        #       Additionally it turns out that it does not only create the DN in LDAP
        #       rather it uploads the entire employee object to LDAP.
        #
        # TODO: Does this upload actively require a cpr_no on the employee?
        #       If we do not have the CPR number nor the ITSystem, we would be leaking
        #       the DN we generate, so maybe we should guard for this, the old code seemed
        #       to do so, maybe we should simply not upload anything in that case.
        dn = await username_generator.generate_dn(employee)

        # If the LDAP ITSystem exists, we want to create a binding to our newly
        # generated (and created) DN, such that it can be correlated in the future.
        #
        # NOTE: This may not be executed if the program crashes after the above line,
        #       thus the current code is not robust and may fail at any time.
        #       The appropriate solution here is to ensure that generate_dn atomically
        #       creates a link between the MO entity and the newly created LDAP entity,
        #       such as by adding the MO UUID to the newly created LDAP entity.
        if raw_it_system_uuid is not None:
            logger.info(
                "No ITUser found, creating one to correlate with DN",
                employee_uuid=uuid,
                dn=dn,
            )
            # Get its unique ldap uuid
            # TODO: Get rid of this code and operate on EntityUUIDs thoughout
            unique_uuid = await self.get_ldap_unique_ldap_uuid(dn)
            logger.info(
                "LDAP UUID found for DN",
                employee_uuid=uuid,
                dn=dn,
                ldap_uuid=unique_uuid,
            )
            # Make a new it-user
            it_user = ITUser.from_simplified_fields(
                str(unique_uuid),
                UUID(raw_it_system_uuid),
                datetime.today().strftime("%Y-%m-%d"),
                person_uuid=uuid,
            )
            # TODO: Convert this to creating the ITUser directly when the modelclient
            #       has been replaced with a GraphQL mutator.
            await self.create([it_user])

        # TODO: What is this purpose of this import, if we just created the DN,
        #       the data should already be up-to-date, no?
        #       It seems weird to synchronize back and forth immediately, but maybe it
        #       is just because the create by generate_dn does not in fact create it
        #       correctly?
        # TODO: Publish this message on the LDAP AMQP exchange
        await self.sync_tool.import_single_user(dn, force=True, manual_import=True)
        await self.graphql_client.employee_refresh(
            self.sync_tool.amqpsystem.exchange_name, [employee.uuid]
        )
        return dn

    async def find_dn_by_engagement_uuid(
        self,
        employee_uuid: UUID,
        engagement: EngagementRef | Engagement | None,
        dns: set[DN],
    ) -> DN:
        # TODO: Should we still validate the DN as we do when we actually look it up?
        if len(dns) == 1:
            return one(dns)
        engagement_uuid: UUID | None = getattr(engagement, "uuid", None)
        ldap_it_system_uuid: UUID = UUID(self.get_ldap_it_system_uuid())

        it_users: list[ITUser] = await self.load_mo_employee_it_users(
            employee_uuid,
            ldap_it_system_uuid,
        )
        matching_it_users: list[ITUser] = [
            it_user
            for it_user in it_users
            if (engagement_uuid is None and it_user.engagement is None)
            or (
                engagement_uuid is not None
                and getattr(it_user.engagement, "uuid", None) == engagement_uuid
            )
        ]

        # TODO: Convert to arguments to one below
        if len(matching_it_users) > 1:
            # Multiple matches
            logger.info(
                "Multiple matches",
                engagement_uuid=engagement_uuid,
                matching_it_users=matching_it_users,
            )
            raise MultipleObjectsReturnedException(
                f"More than one matching 'Unique LDAP UUID' IT user found for "
                f"{employee_uuid=} and {engagement_uuid=}"
            )
        if len(matching_it_users) < 1:
            logger.info(
                "No matches",
                engagement_uuid=engagement_uuid,
                it_users=it_users,
            )
            raise NoObjectsReturnedException("Could not find any matching IT users")

        # Single match, unique ldap UUID is stored in ITUser.user_key
        unique_uuid: UUID = UUID(one(matching_it_users).user_key)
        dn = await self.get_ldap_dn(unique_uuid)
        assert dn in dns
        return dn

    @staticmethod
    def extract_current_or_latest_object(objects: list[dict]):
        """
        Check the validity in a list of object dictionaries and return the one which
        is either valid today, or has the latest end-date
        """
        if len(objects) == 0:
            raise NoObjectsReturnedException("Objects is empty")
        if len(objects) == 1:
            return one(objects)

        def is_current(obj: dict) -> bool:
            valid_to = mo_datestring_to_utc(obj["validity"]["to"])
            valid_from = mo_datestring_to_utc(obj["validity"]["from"])

            now_utc = datetime.utcnow()

            match (valid_from, valid_to):
                case (None, None):
                    return True
                case (start, None):
                    assert start is not None
                    return start < now_utc
                case (None, end):
                    assert end is not None
                    return now_utc < end
                case (start, end):
                    assert start is not None
                    assert end is not None
                    return start < now_utc and now_utc < end
                case _:  # pragma: no cover
                    assert False

        # If any of the objects is valid today, return it
        current_object = only(filter(is_current, objects))
        if current_object:
            return current_object
        # Otherwise return the latest
        latest_object = max(
            objects,
            key=lambda obj: (
                mo_datestring_to_utc(obj["validity"]["to"]) or datetime.max
            ),
        )
        return latest_object

    async def load_mo_employee(self, uuid: UUID, current_objects_only=True) -> Employee:
        start = end = UNSET if current_objects_only else None
        results = await self.graphql_client.read_employees([uuid], start, end)
        result = only(results.objects)
        if result is None:
            raise NoObjectsReturnedException("Could not fetch employee")

        validities = jsonable_encoder(result.validities)
        entry = self.extract_current_or_latest_object(validities)
        entry.pop("validity")
        return Employee(**entry)

    async def load_mo_employees_in_org_unit(self, uuid: OrgUnitUUID) -> list[Employee]:
        """
        Load all current employees engaged to an org unit
        """
        result = await self.graphql_client.read_employees_with_engagement_to_org_unit(
            uuid
        )

        employee_uuids = {
            x.current.employee_uuid for x in result.objects if x.current is not None
        }
        # TODO: dataloader?
        employees = await asyncio.gather(
            *[self.load_mo_employee(employee_uuid) for employee_uuid in employee_uuids]
        )
        return employees

    async def load_mo_facet(self, user_key: str) -> dict[str, Any]:
        result = await self.graphql_client.read_facet_classes(user_key)
        # TODO: Actually return UUID types here
        return {
            str(clazz.current.uuid): jsonable_encoder(clazz.current)
            for clazz in result.objects
            if clazz.current is not None
        }

    async def load_mo_class_uuid(self, user_key: str) -> UUID:
        """Find the UUID of a class by user-key.

        Args:
            user_key: The user-key to lookup.

        Raises:
            MultipleObjectsReturnedException:
                If multiple classes share the same user-key.
            NoObjectsReturnedException:
                If no active classes were found with the user-key.

        Returns:
            The uuid of the corresponding class.
        """
        result = await self.graphql_client.read_class_uuid(user_key)
        too_long = MultipleObjectsReturnedException(
            f"Found multiple classes with user_key = '{user_key}': {result}"
        )
        too_short = NoObjectsReturnedException(
            f"Could not find class with user_key = '{user_key}"
        )
        klass = one(result.objects, too_short=too_short, too_long=too_long)
        return klass.uuid

    async def load_mo_facet_uuid(self, user_key: str) -> UUID:
        """Find the UUID of a facet by user-key.

        Args:
            user_key: The user-key to lookup.

        Raises:
            MultipleObjectsReturnedException:
                If multiple facets share the same user-key.
            NoObjectsReturnedException:
                If no active facets were found with the user-key.

        Returns:
            The uuid of the corresponding facet.
        """
        result = await self.graphql_client.read_facet_uuid(user_key)
        too_long = MultipleObjectsReturnedException(
            f"Found multiple facets with user_key = '{user_key}': {result}"
        )
        too_short = NoObjectsReturnedException(
            f"Could not find facet with user_key = '{user_key}"
        )
        facet = one(result.objects, too_short=too_short, too_long=too_long)
        return facet.uuid

    async def load_mo_employee_address_types(self) -> dict:
        return await self.load_mo_facet("employee_address_type")

    async def load_mo_org_unit_address_types(self) -> dict:
        return await self.load_mo_facet("org_unit_address_type")

    async def load_mo_it_systems(self) -> dict[str, Any]:
        result = await self.graphql_client.read_itsystems()
        # TODO: Actually return UUID types here
        return {
            str(itsystem.current.uuid): jsonable_encoder(itsystem.current)
            for itsystem in result.objects
            if itsystem.current is not None
        }

    async def load_mo_root_org_uuid(self) -> UUID:
        """Get the UUID of the root organisational unit in MO.

        Returns:
            The UUID of the root organisational unit.
        """
        result = await self.graphql_client.read_root_org_uuid()
        return result.uuid

    async def load_mo_org_units(self) -> dict:
        result = await self.graphql_client.read_org_units()
        return {
            str(org_unit.uuid): self.extract_current_or_latest_object(
                jsonable_encoder(org_unit.validities)
            )
            for org_unit in result.objects
        }

    async def load_mo_it_user(self, uuid: UUID, current_objects_only=True) -> ITUser:
        start = end = UNSET if current_objects_only else None
        results = await self.graphql_client.read_itusers([uuid], start, end)
        result = only(results.objects)
        if result is None:
            raise NoObjectsReturnedException("Could not fetch ituser")

        validities = jsonable_encoder(result.validities)
        entry = self.extract_current_or_latest_object(validities)
        return ITUser.from_simplified_fields(
            user_key=entry["user_key"],
            itsystem_uuid=entry["itsystem_uuid"],
            from_date=entry["validity"]["from"],
            uuid=uuid,
            to_date=entry["validity"]["to"],
            person_uuid=entry["employee_uuid"],
            engagement_uuid=entry["engagement_uuid"],
        )

    async def load_mo_address(
        self, uuid: UUID, current_objects_only: bool = True
    ) -> Address:
        """
        Loads a mo address

        Notes
        ---------
        Only returns addresses which are valid today. Meaning the to/from date is valid.
        """
        logger.info("Loading address", uuid=uuid)

        start = end = UNSET if current_objects_only else None
        results = await self.graphql_client.read_addresses([uuid], start, end)
        result = only(results.objects)
        if result is None:
            raise NoObjectsReturnedException("Could not fetch address")

        validities = jsonable_encoder(result.validities)
        entry = self.extract_current_or_latest_object(validities)
        address = Address.from_simplified_fields(
            value=entry["value"],
            address_type_uuid=entry["address_type"]["uuid"],
            from_date=entry["validity"]["from"],
            uuid=entry["uuid"],
            to_date=entry["validity"]["to"],
            value2=entry["value2"],
            person_uuid=entry["employee_uuid"],
            visibility_uuid=entry["visibility_uuid"],
            org_unit_uuid=entry["org_unit_uuid"],
            engagement_uuid=entry["engagement_uuid"],
        )

        return address

    # TODO: Offer this via a dataloader, and change calls to use that
    async def is_primaries(self, engagements: list[UUID]) -> list[bool]:
        engagements_set = set(engagements)
        result = await self.graphql_client.read_is_primary_engagements(
            list(engagements_set)
        )
        result_map = {
            obj.current.uuid: obj.current.is_primary
            for obj in result.objects
            if obj.current is not None
        }
        return [result_map.get(uuid, False) for uuid in engagements]

    # TODO: Offer this via a dataloader, and change calls to use that
    async def is_primary(self, engagement_uuid: UUID) -> bool:
        """
        Determine if an engagement is the primary engagement or not.
        """
        return one(await self.is_primaries([engagement_uuid]))

    async def load_mo_engagement(
        self,
        uuid: UUID,
        current_objects_only: bool = True,
    ) -> Engagement:
        start = end = UNSET if current_objects_only else None
        results = await self.graphql_client.read_engagements([uuid], start, end)
        result = only(results.objects)
        if result is None:
            raise NoObjectsReturnedException("Could not fetch engagement")

        validities = jsonable_encoder(result.validities)
        entry = self.extract_current_or_latest_object(validities)
        engagement = Engagement.from_simplified_fields(
            org_unit_uuid=entry["org_unit_uuid"],
            person_uuid=entry["employee_uuid"],
            job_function_uuid=entry["job_function_uuid"],
            engagement_type_uuid=entry["engagement_type_uuid"],
            user_key=entry["user_key"],
            from_date=entry["validity"]["from"],
            to_date=entry["validity"]["to"],
            uuid=uuid,
            primary_uuid=entry["primary_uuid"],
            extension_1=entry["extension_1"],
            extension_2=entry["extension_2"],
            extension_3=entry["extension_3"],
            extension_4=entry["extension_4"],
            extension_5=entry["extension_5"],
            extension_6=entry["extension_6"],
            extension_7=entry["extension_7"],
            extension_8=entry["extension_8"],
            extension_9=entry["extension_9"],
            extension_10=entry["extension_10"],
        )
        return engagement

    async def load_mo_employee_addresses(
        self, employee_uuid: UUID, address_type_uuid: UUID
    ) -> list[Address]:
        """
        Loads all current addresses of a specific type for an employee
        """
        result = await self.graphql_client.read_employee_addresses(
            employee_uuid, address_type_uuid
        )
        # TODO: Bulk this
        output = await asyncio.gather(
            *[self.load_mo_address(address.uuid) for address in result.objects]
        )
        if not output:
            raise NoObjectsReturnedException(
                "load_mo_employee_addresses returned empty"
            )
        return output

    async def load_mo_org_unit_addresses(
        self, org_unit_uuid: OrgUnitUUID, address_type_uuid: UUID
    ) -> list[Address]:
        """
        Loads all current addresses of a specific type for an org unit
        """
        result = await self.graphql_client.read_org_unit_addresses(
            org_unit_uuid, address_type_uuid
        )
        # TODO: Bulk this
        output = await asyncio.gather(
            *[self.load_mo_address(address.uuid) for address in result.objects]
        )
        if not output:
            raise NoObjectsReturnedException(
                "load_mo_org_unit_addresses returned empty"
            )
        return output

    async def load_mo_employee_it_users(
        self,
        employee_uuid: UUID,
        it_system_uuid: UUID,
    ) -> list[ITUser]:
        """
        Load all current it users of a specific type linked to an employee
        """
        result = await self.graphql_client.read_ituser_by_employee_and_itsystem_uuid(
            employee_uuid, it_system_uuid
        )
        output = await asyncio.gather(
            *[self.load_mo_it_user(ituser.uuid) for ituser in result.objects]
        )
        if not output:
            raise NoObjectsReturnedException("load_mo_employee_it_users returned empty")
        return output

    async def load_mo_employee_engagement_dicts(
        self,
        employee_uuid: UUID,
        user_key: str | None = None,
    ) -> list[dict]:
        filter = EngagementFilter(employee=EmployeeFilter(uuids=[employee_uuid]))
        if user_key is not None:
            filter.user_keys = [user_key]

        result = await self.graphql_client.read_engagements_by_engagements_filter(
            filter
        )
        output = [
            jsonable_encoder(engagement.current)
            for engagement in result.objects
            if engagement.current
        ]
        return output

    async def load_mo_employee_engagements(
        self, employee_uuid: UUID
    ) -> list[Engagement]:
        """
        Load all current engagements linked to an employee
        """
        result = await self.graphql_client.read_engagements_by_employee_uuid(
            employee_uuid
        )
        engagement_uuids = [
            engagement.current.uuid
            for engagement in result.objects
            if engagement.current is not None
        ]
        return await asyncio.gather(*map(self.load_mo_engagement, engagement_uuids))

    async def create_or_edit_mo_objects(self, objects: list[tuple[MOBase, Verb]]):
        def star(func):
            @wraps(func)
            def wrapper(tup: tuple) -> Any:
                return func(*tup)

            return wrapper

        def fix_verb(obj: MOBase, verb: Verb) -> tuple[MOBase, Verb]:
            if hasattr(obj, "terminate_"):
                return obj, Verb.TERMINATE
            return obj, verb

        # HACK to set termination verb, should be set within format_converted_objects instead,
        # but doing so requires restructuring the entire flow of the integration, which is a major
        # task best saved for later.
        objects = [fix_verb(obj, verb) for obj, verb in objects]

        # Split objects into groups
        verb_groups = bucket(objects, key=star(lambda _, verb: verb))
        creates = verb_groups[Verb.CREATE]
        edits = verb_groups[Verb.EDIT]
        terminates = verb_groups[Verb.TERMINATE]

        create_results, edit_results, terminate_results = await asyncio.gather(
            self.create([obj for obj, _ in creates]),
            self.edit([obj for obj, _ in edits]),
            self.terminate([obj for obj, _ in terminates]),
        )
        return cast(list[Any | None], create_results + edit_results + terminate_results)

    async def create(self, creates: list[MOBase]) -> list[Any]:
        model_client = self.context["legacy_model_client"]
        return cast(list[Any], await model_client.upload(creates))

    async def edit(self, edits: list[MOBase]) -> list[Any]:
        model_client = self.context["legacy_model_client"]
        return cast(list[Any], await model_client.edit(edits))

    async def terminate_address(self, uuid: UUID, at: datetime) -> UUID:
        result = await self.graphql_client.address_terminate(
            AddressTerminateInput(uuid=uuid, to=at)
        )
        return result.uuid

    async def terminate_engagement(self, uuid: UUID, at: datetime) -> UUID:
        result = await self.graphql_client.engagement_terminate(
            EngagementTerminateInput(uuid=uuid, to=at)
        )
        return result.uuid

    async def terminate_ituser(self, uuid: UUID, at: datetime) -> UUID:
        result = await self.graphql_client.ituser_terminate(
            ITUserTerminateInput(uuid=uuid, to=at)
        )
        return result.uuid

    async def terminate_object(self, uuid: UUID, at: datetime, motype: str) -> UUID:
        """Terminate a detail.

        This method calls the appropriate `terminate_x` method to terminate the object.

        Args:
            terminatee: The detail to terminate

        Returns:
            UUID of the terminated entry
        """

        match motype:
            case "address":
                return await self.terminate_address(uuid, at)
            case "engagement":
                return await self.terminate_engagement(uuid, at)
            case "it":
                return await self.terminate_ituser(uuid, at)
            case _:
                raise ValueError(f"Unable to terminate type: {motype}")

    async def terminate(self, terminatees: list[Any]) -> list[UUID]:
        """Terminate a list of details.

        This method calls `terminate_object` for each objects in parallel.

        Args:
            terminatees: The list of details to terminate.

        Returns:
            UUIDs of the terminated entries
        """

        def is_exception(x: Any) -> bool:
            return isinstance(x, Exception)

        detail_terminations: list[dict[str, Any]] = [
            {
                "motype": terminate.type_,
                "uuid": terminate.uuid,
                "at": terminate.terminate_,
            }
            for terminate in terminatees
        ]
        tasks = [self.terminate_object(**detail) for detail in detail_terminations]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        exceptions = cast(list[Exception], list(filter(is_exception, results)))
        if exceptions:
            raise ExceptionGroup("Exceptions during termination", exceptions)
        return cast(list[UUID], results)

    async def create_mo_class(
        self,
        name: str,
        user_key: str,
        facet_uuid: UUID,
        scope: str | None = None,
    ) -> UUID:
        """Creates a class in MO.

        Args:
            name: The name for the class.
            user_key: The user-key for the class.
            facet_uuid: The UUID of the facet to attach this class to.
            scope: The optional scope to assign to the class.

        Returns:
            The uuid of the existing or newly created class.
        """
        async with self.create_mo_class_lock:
            # If class already exists, noop
            with suppress(NoObjectsReturnedException):
                uuid = await self.load_mo_class_uuid(user_key)
                logger.info("MO class exists", user_key=user_key)
                return uuid

            logger.info("Creating MO class", user_key=user_key)
            input = ClassCreateInput(
                name=name,
                user_key=user_key,
                facet_uuid=facet_uuid,
                scope=scope,
                validity=RAOpenValidityInput(from_=None),
            )
            result = await self.graphql_client.class_create(input)
            return result.uuid

    async def update_mo_class(
        self,
        name: str,
        user_key: str,
        facet_uuid: UUID,
        class_uuid: UUID,
        scope: str | None = None,
    ) -> UUID:
        """Updates a class in MO.

        Args:
            name: The name for the class.
            user_key: The user-key for the class.
            facet_uuid: The UUID of the facet to attach this class to.
            class_uuid: The UUID of the class to update.
            scope: The optional scope to assign to the class.

        Returns:
            The uuid of the updated class (equal to the class_uuid input).
        """
        logger.info("Modifying MO class", user_key=user_key)
        input = ClassUpdateInput(
            uuid=class_uuid,
            name=name,
            user_key=user_key,
            facet_uuid=facet_uuid,
            scope=scope,
            validity=RAOpenValidityInput(from_=None),
        )
        result = await self.graphql_client.class_update(input)
        return result.uuid

    async def create_mo_job_function(self, name) -> UUID:
        """
        Creates a job function class in MO

        Returns
        ----------
        uuid: UUID
            The uuid of the created class
        """
        logger.info("Creating MO job function", name=name)
        facet_uuid = await self.load_mo_facet_uuid("engagement_job_function")
        user_key = name
        return await self.create_mo_class(name, user_key, facet_uuid)

    async def create_mo_engagement_type(self, name) -> UUID:
        """
        Creates an engagement type class in MO

        Returns
        ----------
        uuid: UUID
            The uuid of the created class
        """
        logger.info("Creating MO engagement type", name=name)
        facet_uuid = await self.load_mo_facet_uuid("engagement_type")
        user_key = name
        return await self.create_mo_class(name, user_key, facet_uuid)

    async def create_mo_it_system(self, name: str, user_key: str) -> UUID:
        """Creates an it-system in MO.

        Args:
            name: The name for the it-system.
            user_key: The user-key for the it-system.

        Returns:
            The uuid of the created it-system.
        """
        logger.info("Creating MO it-system", user_key=user_key)
        input = ITSystemCreateInput(
            name=name, user_key=user_key, validity=RAOpenValidityInput(from_=None)
        )
        result = await self.graphql_client.itsystem_create(input)
        return result.uuid
